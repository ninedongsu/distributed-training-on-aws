## 2. ParallelCluster ë°°í¬

> ğŸ’¡ **ëª©í‘œ:** AWS ParallelClusterë¥¼ ìƒì„±í•˜ê³  Slurm ìŠ¤ì¼€ì¤„ëŸ¬ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.

â±ï¸ **ì˜ˆìƒ ì†Œìš” ì‹œê°„:** 30-40ë¶„

## ëª©ì°¨

- [ê°œìš”](#ê°œìš”)
- [2.1 í´ëŸ¬ìŠ¤í„° ì„¤ì • íŒŒì¼ ì¤€ë¹„](#21-í´ëŸ¬ìŠ¤í„°-ì„¤ì •-íŒŒì¼-ì¤€ë¹„)
- [2.2 í´ëŸ¬ìŠ¤í„° ìƒì„±](#22-í´ëŸ¬ìŠ¤í„°-ìƒì„±)
- [2.3 í´ëŸ¬ìŠ¤í„° ì ‘ì† ë° ê²€ì¦](#23-í´ëŸ¬ìŠ¤í„°-ì ‘ì†-ë°-ê²€ì¦)
- [2.4 Slurm ê¸°ë³¸ ì‚¬ìš©](#24-slurm-ê¸°ë³¸-ì‚¬ìš©)
- [ë‹¤ìŒ ë‹¨ê³„](#ë‹¤ìŒ-ë‹¨ê³„)

---

## ê°œìš”

ì´ ë¬¸ì„œì—ì„œëŠ” ë‹¤ìŒ ì‘ì—…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤:

- âœ… ParallelCluster ì„¤ì • íŒŒì¼ ì‘ì„±
- âœ… í´ëŸ¬ìŠ¤í„° ìƒì„± ë° ê²€ì¦
- âœ… Slurm ê¸°ë³¸ ëª…ë ¹ì–´ ì‚¬ìš© (nvidia-smi w/ srun)

---

## 2.1 í´ëŸ¬ìŠ¤í„° ì„¤ì • íŒŒì¼ ì¤€ë¹„

ParallelCluster ìƒì„±ì„ ìœ„í•œ YAML ì„¤ì • íŒŒì¼ì„ ì¤€ë¹„í•©ë‹ˆë‹¤.

### í™˜ê²½ ë³€ìˆ˜ í™•ì¸

ë¨¼ì € ì´ì „ ë‹¨ê³„ì—ì„œ ì„¤ì •í•œ í™˜ê²½ ë³€ìˆ˜ë“¤ì´ ë¡œë“œë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤:

```bash
# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
source ~/pcluster-env.sh

# ì£¼ìš” ë³€ìˆ˜ í™•ì¸
echo "Region: ${AWS_REGION}"
echo "VPC ID: ${VPC_ID}"
echo "Public Subnet: ${PUBLIC_SUBNET_ID}"
echo "Private Subnet: ${PRIVATE_SUBNET_ID}"
echo "Security Group: ${SECURITY_GROUP_ID}"
echo "FSx Lustre ID: ${FSX_LUSTRE_ID}"
echo "FSx OpenZFS Volume ID: ${FSX_OPENZFS_ROOT_VOLUME_ID}"
echo "Head Node Bootstrap: ${HEAD_NODE_BOOTSTRAP_SCRIPT}"
echo "Compute Node Bootstrap: ${COMPUTE_NODE_BOOTSTRAP_SCRIPT}"
```

### í´ëŸ¬ìŠ¤í„° ì„¤ì • íŒŒì¼ ìƒì„±

í™˜ê²½ ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ í´ëŸ¬ìŠ¤í„° ì„¤ì • íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤:

> ğŸ“ **í…œí”Œë¦¿ íŒŒì¼ ì°¸ì¡°:**
> - ì „ì²´ ì„¤ì • íŒŒì¼ í…œí”Œë¦¿ì€ [examples/templates/cluster-config.yaml.template](../examples/templates/cluster-config.yaml.template)ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
> - ì•„ë˜ ëª…ë ¹ì€ í™˜ê²½ ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ í…œí”Œë¦¿ì„ ì‹¤ì œ ì„¤ì • íŒŒì¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

```bash
# ì„¤ì • íŒŒì¼ ë””ë ‰í† ë¦¬ë¡œ ì´ë™
cd ~/distributed-training-on-aws/pcluster-container
mkdir -p examples/configs

# ì¸ìŠ¤í„´ìŠ¤ íƒ€ì… ë° ìˆ˜ëŸ‰ ì„¤ì • (í•„ìš”ì‹œ ë³€ê²½)
export COMPUTE_INSTANCE_TYPE=g5.8xlarge
export COMPUTE_MIN_COUNT=2
export COMPUTE_MAX_COUNT=2

# í´ëŸ¬ìŠ¤í„° ì„¤ì • íŒŒì¼ ìƒì„±
cat > examples/configs/cluster-config.yaml << EOF
# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: MIT-0
Region: ${AWS_REGION}

DevSettings:
  Timeouts:
    HeadNodeBootstrapTimeout: 43200  # 12 hours
    ComputeNodeBootstrapTimeout: 7200  # 2 hours

Imds:
  ImdsSupport: v2.0

Image:
  Os: ubuntu2204

HeadNode:
  InstanceType: m5.8xlarge
  Networking:
    SubnetId: ${PUBLIC_SUBNET_ID}
    ElasticIp: false
    AdditionalSecurityGroups:
      - ${SECURITY_GROUP_ID}
  LocalStorage:
    RootVolume:
      Size: 500
      DeleteOnTermination: true  # Root and /home volume for users
  Iam:
    AdditionalIamPolicies:
      # Grant ECR, SSM and S3 access
      - Policy: arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore
      - Policy: arn:aws:iam::aws:policy/AmazonS3FullAccess
      - Policy: arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryFullAccess
      - Policy: arn:aws:iam::aws:policy/AmazonElasticContainerRegistryPublicFullAccess
  CustomActions:
    OnNodeConfigured:
      Sequence:
        - Script: 'https://raw.githubusercontent.com/aws-samples/aws-parallelcluster-post-install-scripts/main/docker/postinstall.sh'
        - Script: '${HEAD_NODE_BOOTSTRAP_SCRIPT}'

Scheduling:
  Scheduler: slurm
  SlurmSettings:
    ScaledownIdletime: -1  # Disable automatic scale-down
    QueueUpdateStrategy: DRAIN
    CustomSlurmSettings:
      # Simple accounting to text file
      - JobCompType: jobcomp/filetxt
      - JobCompLoc: /home/slurm/slurm-job-completions.txt
      - JobAcctGatherType: jobacct_gather/linux
      # Increase timeout before marking node as DOWN
      - SlurmdTimeout: 1000
  SlurmQueues:
    - Name: compute-gpu
      CapacityType: ONDEMAND
      Networking:
        SubnetIds:
          - ${PRIVATE_SUBNET_ID}
        PlacementGroup:
          Enabled: true  # Capacity Reservation ì‚¬ìš© ì‹œ falseë¡œ ë³€ê²½
        AdditionalSecurityGroups:
          - ${SECURITY_GROUP_ID}
      ComputeSettings:
        LocalStorage:
          EphemeralVolume:
            MountDir: /scratch  # Local NVMe scratch space
          RootVolume:
            Size: 512
      JobExclusiveAllocation: true  # Each job gets exclusive access to nodes
      ComputeResources:
        - Name: distributed-ml
          InstanceType: ${COMPUTE_INSTANCE_TYPE}
          MinCount: ${COMPUTE_MIN_COUNT}
          MaxCount: ${COMPUTE_MAX_COUNT}
          # Capacity Reservation ì‚¬ìš© ì‹œ ì•„ë˜ ì£¼ì„ í•´ì œ
          # CapacityReservationTarget:
          #   CapacityReservationId: cr-0a1f6b92ded769450  # Replace with your Capacity Reservation ID
          Efa:
            Enabled: true
            #GdrSupport: true  # p4d/p5 ì¸ìŠ¤í„´ìŠ¤ë§Œ ì§€ì›
      Iam:
        AdditionalIamPolicies:
          - Policy: arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore
          - Policy: arn:aws:iam::aws:policy/AmazonS3FullAccess
          - Policy: arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryFullAccess
          - Policy: arn:aws:iam::aws:policy/AmazonElasticContainerRegistryPublicFullAccess
      CustomActions:
        OnNodeConfigured:
          Sequence:
            - Script: 'https://raw.githubusercontent.com/aws-samples/aws-parallelcluster-post-install-scripts/main/docker/postinstall.sh'
            - Script: '${COMPUTE_NODE_BOOTSTRAP_SCRIPT}'

SharedStorage:
  - Name: shared-workspace-zfs
    StorageType: FsxOpenZfs
    MountDir: /fsx
    FsxOpenZfsSettings:
      VolumeId: ${FSX_OPENZFS_ROOT_VOLUME_ID}

  - Name: fsx-lustre
    MountDir: /lustre
    StorageType: FsxLustre
    FsxLustreSettings:
      FileSystemId: ${FSX_LUSTRE_ID}

Monitoring:
  DetailedMonitoring: true
  Logs:
    CloudWatch:
      Enabled: true
  Dashboards:
    CloudWatch:
      Enabled: true
EOF

echo "âœ… Cluster configuration file created: examples/configs/cluster-config.yaml"
```

### ì„¤ì • íŒŒì¼ ì£¼ìš” êµ¬ì„± ìš”ì†Œ

**HeadNode ì„¤ì •:**
- **InstanceType**: `m5.8xlarge` - 32 vCPU, 128GB RAM (Slurm ì»¨íŠ¸ë¡¤ëŸ¬ìš©)
- **SubnetId**: Public ì„œë¸Œë„· ì‚¬ìš© (Session Manager ì ‘ì†ìš©)
- **RootVolume**: 500GB (ì‚¬ìš©ì í™ˆ ë””ë ‰í† ë¦¬ í¬í•¨)
- **CustomActions**: Docker ë° Enroot/Pyxis ì„¤ì¹˜ ìŠ¤í¬ë¦½íŠ¸

**Compute Node ì„¤ì •:**
- **InstanceType**: í™˜ê²½ ë³€ìˆ˜ë¡œ ì„¤ì • (ê¸°ë³¸: `g5.8xlarge`)
- **SubnetId**: Private ì„œë¸Œë„·
- **MinCount/MaxCount**: í™˜ê²½ ë³€ìˆ˜ë¡œ ì¡°ì • ê°€ëŠ¥
- **EFA**: í™œì„±í™” (ê³ ì„±ëŠ¥ ë„¤íŠ¸ì›Œí‚¹)
- **LocalStorage**: 
  - `/scratch`: NVMe ì„ì‹œ ìŠ¤í† ë¦¬ì§€
  - Root Volume: 512GB
- **CustomActions**: Docker ë° Enroot/Pyxis ì„¤ì¹˜ ìŠ¤í¬ë¦½íŠ¸

**SharedStorage:**
- **FSx OpenZFS** (`/fsx`): Home ë””ë ‰í† ë¦¬ ë° ì‚¬ìš©ì ë°ì´í„°
- **FSx Lustre** (`/lustre`): í•™ìŠµ ë°ì´í„°ì…‹
  - ê¸°ì¡´ FSx Lustre íŒŒì¼ ì‹œìŠ¤í…œì„ ì‚¬ìš© (`FileSystemId`ë¡œ ì°¸ì¡°)
  - **Data Repository Association (DRA)**ì€ ì´ë¯¸ 01-prerequisites.mdì—ì„œ ì„¤ì •ë¨
  - í´ëŸ¬ìŠ¤í„°ê°€ ë§ˆìš´íŠ¸í•˜ë©´ ê¸°ì¡´ DRA ì„¤ì •ì´ ìë™ìœ¼ë¡œ ì ìš©ë¨:
    - `/lustre/data` â†” `s3://${S3_BUCKET_NAME}/data/`
    - `/lustre/checkpoints` â†” `s3://${S3_BUCKET_NAME}/checkpoints/`
    - `/lustre/logs` â†” `s3://${S3_BUCKET_NAME}/logs/`
    - `/lustre/results` â†” `s3://${S3_BUCKET_NAME}/results/`

> ğŸ’¡ **DRAëŠ” FSx ë ˆë²¨ì˜ ì„¤ì •**ì´ë¯€ë¡œ ParallelCluster YAMLì—ì„œ ë³„ë„ ì„¤ì •ì´ í•„ìš” ì—†ìŠµë‹ˆë‹¤.

---

### Capacity Reservation ì‚¬ìš© (ì„ íƒì‚¬í•­)

GPU ì¸ìŠ¤í„´ìŠ¤ ê°€ìš©ì„±ì„ ë³´ì¥í•˜ê¸° ìœ„í•´ Capacity Reservationì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

#### Capacity Reservation ìƒì„± (AWS Console ë˜ëŠ” CLI)

```bash
# Capacity Reservation ìƒì„± ì˜ˆì‹œ
aws ec2 create-capacity-reservation \
  --instance-type g5.12xlarge \
  --instance-platform Linux/UNIX \
  --availability-zone ${PRIMARY_AZ} \
  --instance-count 2 \
  --instance-match-criteria targeted \
  --region ${AWS_REGION}
```

#### ì„¤ì • íŒŒì¼ ìˆ˜ì •

Capacity Reservationì„ ì‚¬ìš©í•˜ë ¤ë©´ ì„¤ì • íŒŒì¼ì—ì„œ ë‹¤ìŒì„ ìˆ˜ì •:

1. **PlacementGroup ë¹„í™œì„±í™”**:
```yaml
PlacementGroup:
  Enabled: false  # Capacity Reservationê³¼ í•¨ê»˜ ì‚¬ìš© ë¶ˆê°€
```

2. **CapacityReservationTarget ì¶”ê°€**:
```yaml
ComputeResources:
  - Name: distributed-ml
    InstanceType: ${COMPUTE_INSTANCE_TYPE}
    MinCount: 2  # Reservation ìˆ˜ëŸ‰ê³¼ ì¼ì¹˜
    MaxCount: 2
    CapacityReservationTarget:
      CapacityReservationId: cr-0123456789abcdef0  # ì‹¤ì œ IDë¡œ ë³€ê²½
```

> âš ï¸ **ì£¼ì˜:** Capacity Reservation ì‚¬ìš© ì‹œ MinCountë¥¼ Reservation ìˆ˜ëŸ‰ì— ë§ì¶° ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤.

---

### ì„¤ì • íŒŒì¼ í™•ì¸

ìƒì„±ëœ ì„¤ì • íŒŒì¼ì„ í™•ì¸í•©ë‹ˆë‹¤:

```bash
# ì„¤ì • íŒŒì¼ ë‚´ìš© í™•ì¸
cat examples/configs/cluster-config.yaml

> âš ï¸ **ì£¼ì˜:**
> - í™˜ê²½ ë³€ìˆ˜ê°€ ì œëŒ€ë¡œ ì¹˜í™˜ë˜ì—ˆëŠ”ì§€ ê¼­ í™•ì¸í•˜ì„¸ìš”.

---

## 2.2 í´ëŸ¬ìŠ¤í„° ìƒì„±

### í´ëŸ¬ìŠ¤í„° ì´ë¦„ ì„¤ì •

```bash
export CLUSTER_NAME=ml-training-cluster
```

### í´ëŸ¬ìŠ¤í„° ìƒì„± ì‹œì‘

```bash
# í´ëŸ¬ìŠ¤í„° ìƒì„±
pcluster create-cluster \
  --cluster-name ${CLUSTER_NAME} \
  --cluster-configuration examples/configs/cluster-config.yaml \
  --region ${AWS_REGION}
```

**ì˜ˆìƒ ì¶œë ¥:**
```json
{
  "cluster": {
    "clusterName": "ml-training-cluster",
    "cloudformationStackStatus": "CREATE_IN_PROGRESS",
    "cloudformationStackArn": "arn:aws:cloudformation:us-east-1:123456789012:stack/ml-training-cluster/883840a0-cd49-11f0-8ba1-0edd122729eb",
    "region": "us-east-1",
    "version": "3.14.0",
    "clusterStatus": "CREATE_IN_PROGRESS",
    "scheduler": {
      "type": "slurm"
    }
  },
  "validationMessages": [
    {
      "level": "WARNING",
      "type": "DetailedMonitoringValidator",
      "message": "Detailed Monitoring is enabled for EC2 instances in your compute fleet. The Amazon EC2 console will display monitoring graphs with a 1-minute period for these instances. Note that this will increase the cost. If you want to avoid this and use basic monitoring instead, please set `Monitoring / DetailedMonitoring` to false."
    },
    {
      "level": "WARNING",
      "type": "KeyPairValidator",
      "message": "If you do not specify a key pair, you can't connect to the instance unless you choose an AMI that is configured to allow users another way to log in"
    }
  ]
}
```

### í´ëŸ¬ìŠ¤í„° ìƒì„± ìƒíƒœ ëª¨ë‹ˆí„°ë§

```bash
# í´ëŸ¬ìŠ¤í„° ìƒíƒœ í™•ì¸
pcluster describe-cluster \
  --cluster-name ${CLUSTER_NAME} \
  --region ${AWS_REGION} \
  --query 'clusterStatus'
```

**ì˜ˆìƒ ìƒíƒœ:**
- `CREATE_IN_PROGRESS`: ìƒì„± ì¤‘
- `CREATE_COMPLETE`: ìƒì„± ì™„ë£Œ âœ…
- `CREATE_FAILED`: ìƒì„± ì‹¤íŒ¨ âŒ

> â±ï¸ **ì˜ˆìƒ ì†Œìš” ì‹œê°„: ì•½ 25-35ë¶„**

### CloudFormation ìŠ¤íƒ í™•ì¸

```bash
# CloudFormation ìŠ¤íƒ ì´ë²¤íŠ¸ í™•ì¸
aws cloudformation describe-stack-events \
  --stack-name ${CLUSTER_NAME} \
  --region ${AWS_REGION} \
  --max-items 10 \
  --query 'StackEvents[*].[Timestamp,ResourceStatus,ResourceType,LogicalResourceId]' \
  --output table
```

---

## 2.3 í´ëŸ¬ìŠ¤í„° ì ‘ì† ë° ê²€ì¦

í´ëŸ¬ìŠ¤í„° ìƒì„±ì´ ì™„ë£Œë˜ë©´ Head Nodeì— ì ‘ì†í•˜ì—¬ í™˜ê²½ì„ í™•ì¸í•©ë‹ˆë‹¤.

### Head Node ì ‘ì†

Session Managerë¥¼ í†µí•´ Head Nodeì— ì ‘ì†í•©ë‹ˆë‹¤:

```bash
# SSH ì ‘ì†
pcluster ssh \
  --cluster-name ${CLUSTER_NAME} \
  --region ${AWS_REGION}
```

**ì˜ˆìƒ ì¶œë ¥:**
```
Starting session with SessionId: user-0a1b2c3d4e5f6g7h8

       __|  __|_  )
       _|  (     /   Amazon Linux 2
      ___|\___|___|

ubuntu@ip-10-0-0-123:~$
```

> ğŸ’¡ **Session Manager ì‚¬ìš©:**
> - SSH í‚¤ ì—†ì´ ì•ˆì „í•˜ê²Œ ì ‘ì†
> - IAM ê¸°ë°˜ ì¸ì¦
> - ì„¸ì…˜ ë¡œê·¸ ìë™ ê¸°ë¡

### ê¸°ë³¸ í™˜ê²½ í™•ì¸

Head Nodeì— ì ‘ì†í•œ í›„ ë‹¤ìŒ ëª…ë ¹ìœ¼ë¡œ í™˜ê²½ì„ í™•ì¸í•©ë‹ˆë‹¤:

#### OS ë° ì‹œìŠ¤í…œ ì •ë³´

```bash
# OS ì •ë³´
cat /etc/os-release

# ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤
free -h
df -h
```

**ì˜ˆìƒ ì¶œë ¥:**
```
NAME="Ubuntu"
VERSION="22.04.x LTS (Jammy Jellyfish)"
ID=ubuntu
ID_LIKE=debian

              total        used        free      shared  buff/cache   available
Mem:          125Gi       2.5Gi       120Gi       1.0Mi       2.8Gi       122Gi
Swap:            0B          0B          0B
```

#### ê³µìœ  ìŠ¤í† ë¦¬ì§€ í™•ì¸

```bash
# ë§ˆìš´íŠ¸ëœ ê³µìœ  ìŠ¤í† ë¦¬ì§€ í™•ì¸
df -h | grep -E 'fsx|lustre'

# ë˜ëŠ” ì „ì²´ ë§ˆìš´íŠ¸ í™•ì¸
mount | grep -E 'fsx|lustre'
```

**ì˜ˆìƒ ì¶œë ¥:**
```
10.0.1.100@tcp:/fsvol-xxx  512G   64M  512G   1% /fsx
10.0.1.101@tcp:/yyyyyyy    1.2T  1.1M  1.2T   1% /lustre
```

#### FSx Lustre ë””ë ‰í† ë¦¬ êµ¬ì¡° í™•ì¸

```bash
# Lustre ë””ë ‰í† ë¦¬ í™•ì¸
ls -la /lustre/

# DRAë¡œ ì—°ê²°ëœ ë””ë ‰í† ë¦¬ í™•ì¸
ls -la /lustre/data/
ls -la /lustre/checkpoints/
ls -la /lustre/logs/
ls -la /lustre/results/
```

**ì˜ˆìƒ ì¶œë ¥:**
```
total 16
drwxr-xr-x  6 root root 4096 Nov 29 16:00 .
drwxr-xr-x 23 root root 4096 Nov 29 17:00 ..
drwxr-xr-x  3 root root 4096 Nov 29 16:50 data
drwxr-xr-x  2 root root 4096 Nov 29 16:17 checkpoints
drwxr-xr-x  2 root root 4096 Nov 29 16:17 logs
drwxr-xr-x  2 root root 4096 Nov 29 16:17 results
```

#### WikiText-2 ë°ì´í„°ì…‹ í™•ì¸

```bash
# 01-prerequisites.mdì—ì„œ ì—…ë¡œë“œí•œ ë°ì´í„° í™•ì¸
ls -lh /lustre/data/wikitext-2/
```

**ì˜ˆìƒ ì¶œë ¥:**
```
total 0
-rw-r--r-- 1 root root   43 Nov 29 16:49 dataset_dict.json
drwxr-xr-x 2 root root 4.0K Nov 29 16:49 test
drwxr-xr-x 2 root root 4.0K Nov 29 16:49 train
drwxr-xr-x 2 root root 4.0K Nov 29 16:49 validation
```

> ğŸ’¡ **Lazy Loading:** íŒŒì¼ ë©”íƒ€ë°ì´í„°ëŠ” ì¦‰ì‹œ ë³´ì´ì§€ë§Œ, ì‹¤ì œ ë°ì´í„°ëŠ” íŒŒì¼ ì ‘ê·¼ ì‹œ S3ì—ì„œ ë¡œë“œë©ë‹ˆë‹¤.

#### Docker í™•ì¸

```bash
# Docker ë²„ì „ í™•ì¸
docker --version

# Docker ì„œë¹„ìŠ¤ ìƒíƒœ
sudo systemctl status docker
```

**ì˜ˆìƒ ì¶œë ¥:**
```
Docker version 24.0.7, build afdd53b
â— docker.service - Docker Application Container Engine
     Loaded: loaded (/lib/systemd/system/docker.service; enabled)
     Active: active (running)
```

#### Enroot í™•ì¸

```bash
# Enroot ë²„ì „ í™•ì¸
enroot version

# Enroot ì„¤ì • í™•ì¸
enroot list
```

**ì˜ˆìƒ ì¶œë ¥:**
```
3.4.1
```

#### Pyxis í™•ì¸

```bash
# Pyxis í”ŒëŸ¬ê·¸ì¸ í™•ì¸
ls -la /usr/local/lib/slurm/

# Slurm ì„¤ì •ì—ì„œ Pyxis í™•ì¸
grep -i pyxis /opt/slurm/etc/plugstack.conf
```

**ì˜ˆìƒ ì¶œë ¥:**
```
total 24
drwxr-xr-x 2 root root  4096 Nov 29 17:15 .
drwxr-xr-x 4 root root  4096 Nov 29 17:00 ..
-rwxr-xr-x 1 root root 14896 Nov 29 17:15 spank_pyxis.so

optional /usr/local/lib/slurm/spank_pyxis.so
```

---

## 2.4 Slurm ê¸°ë³¸ ì‚¬ìš©

### Slurm ë…¸ë“œ ìƒíƒœ í™•ì¸

```bash
# ë…¸ë“œ ì •ë³´ í™•ì¸
sinfo
```

**ì˜ˆìƒ ì¶œë ¥:**
```
PARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST
compute-gpu*  up   infinite      2  idle~ compute-gpu-distributed-ml-[1-4]
```

> ğŸ“ **ë…¸ë“œ ìƒíƒœ:**
> - `idle~`: ìœ íœ´ ìƒíƒœ, í•„ìš” ì‹œ ìë™ í”„ë¡œë¹„ì €ë‹
> - `alloc`: ì‘ì—…ì— í• ë‹¹ë¨
> - `mix`: ì¼ë¶€ ë¦¬ì†ŒìŠ¤ ì‚¬ìš© ì¤‘
> - `down`: ì‚¬ìš© ë¶ˆê°€

### Slurm íŒŒí‹°ì…˜ í™•ì¸

```bash
# íŒŒí‹°ì…˜ ì •ë³´
scontrol show partition compute-gpu
```

### ê°„ë‹¨í•œ nvidia-smi í…ŒìŠ¤íŠ¸

Compute Nodeë¥¼ í”„ë¡œë¹„ì €ë‹í•˜ê³  GPUë¥¼ í™•ì¸í•˜ëŠ” ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤:

```bash
# nvidia-smi í…ŒìŠ¤íŠ¸ ì‘ì—… ì œì¶œ
srun --partition=compute-gpu \
     --nodes=1 \
     --ntasks=1 \
     --gpus-per-node=1 \
     nvidia-smi
```

**ì˜ˆìƒ ì¶œë ¥:**
```
TBU
```

### ì‘ì—… í í™•ì¸

```bash
# ì‹¤í–‰ ì¤‘ì¸ ì‘ì—… í™•ì¸
squeue

# ë³¸ì¸ì˜ ì‘ì—…ë§Œ í™•ì¸
squeue -u $USER

# ì‘ì—… ìƒì„¸ ì •ë³´
scontrol show job <JOB_ID>
```

### Compute Node ìƒíƒœ í™•ì¸

```bash
# í”„ë¡œë¹„ì €ë‹ëœ ë…¸ë“œ í™•ì¸
sinfo

# íŠ¹ì • ë…¸ë“œ ìƒì„¸ ì •ë³´
scontrol show node compute-gpu-distributed-ml-1
```

---

## ë‹¤ìŒ ë‹¨ê³„

âœ… ParallelCluster ë°°í¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!

ì´ì œ **[3. ë¶„ì‚° í•™ìŠµ ì‹¤í–‰](./03-distributed-training.md)**ìœ¼ë¡œ ì§„í–‰í•˜ì—¬ ì‹¤ì œ í•™ìŠµ ì‘ì—…ì„ ì‹¤í–‰í•˜ì„¸ìš”.

---

### í™˜ê²½ ë³€ìˆ˜ ì €ì¥

```bash
# í´ëŸ¬ìŠ¤í„° ì´ë¦„ ì €ì¥
cat >> ~/pcluster-env.sh << EOF
export CLUSTER_NAME=${CLUSTER_NAME}
EOF
```

---

## ğŸ“š ë„¤ë¹„ê²Œì´ì…˜

| ì´ì „ | ìƒìœ„ | ë‹¤ìŒ |
|------|------|------|
| [â—€ ì‚¬ì „ ìš”êµ¬ì‚¬í•­](./01-prerequisites.md) | [ğŸ“‘ ëª©ì°¨](../README.md#-ê°€ì´ë“œ-ëª©ì°¨) | [ë¶„ì‚° í•™ìŠµ â–¶](./03-distributed-training.md) |