## 2. ParallelCluster ë°°í¬

> ðŸ’¡ **ëª©í‘œ:** AWS ParallelClusterë¥¼ ìƒì„±í•˜ê³  Slurm ìŠ¤ì¼€ì¤„ëŸ¬ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.

â±ï¸ **ì˜ˆìƒ ì†Œìš” ì‹œê°„:** 30-40ë¶„

## ëª©ì°¨

- [ê°œìš”](#ê°œìš”)
- [2.1 í´ëŸ¬ìŠ¤í„° ì„¤ì • íŒŒì¼ ì¤€ë¹„](#21-í´ëŸ¬ìŠ¤í„°-ì„¤ì •-íŒŒì¼-ì¤€ë¹„)
- [2.2 í´ëŸ¬ìŠ¤í„° ìƒì„±](#22-í´ëŸ¬ìŠ¤í„°-ìƒì„±)
- [2.3 í´ëŸ¬ìŠ¤í„° ì ‘ì† ë° ê²€ì¦](#23-í´ëŸ¬ìŠ¤í„°-ì ‘ì†-ë°-ê²€ì¦)
- [2.4 Slurm ê¸°ë³¸ ì‚¬ìš©](#24-slurm-ê¸°ë³¸-ì‚¬ìš©)
- [ë‹¤ìŒ ë‹¨ê³„](#ë‹¤ìŒ-ë‹¨ê³„)

---

## ê°œìš”

ì´ ë¬¸ì„œì—ì„œëŠ” ë‹¤ìŒ ìž‘ì—…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤:

- âœ… ParallelCluster ì„¤ì • íŒŒì¼ ìž‘ì„±
- âœ… í´ëŸ¬ìŠ¤í„° ìƒì„± ë° ê²€ì¦
- âœ… Slurm ê¸°ë³¸ ëª…ë ¹ì–´ ì‚¬ìš© (nvidia-smi w/ srun)

---

## 2.1 í´ëŸ¬ìŠ¤í„° ì„¤ì • íŒŒì¼ ì¤€ë¹„

ParallelCluster ìƒì„±ì„ ìœ„í•œ YAML ì„¤ì • íŒŒì¼ì„ ì¤€ë¹„í•©ë‹ˆë‹¤.

### í™˜ê²½ ë³€ìˆ˜ í™•ì¸

ë¨¼ì € ì´ì „ ë‹¨ê³„ì—ì„œ ì„¤ì •í•œ í™˜ê²½ ë³€ìˆ˜ë“¤ì´ ë¡œë“œë˜ì–´ ìžˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤:

```bash
# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
source ~/pcluster-env.sh

# ì£¼ìš” ë³€ìˆ˜ í™•ì¸
echo "Region: ${AWS_REGION}"
echo "VPC ID: ${VPC_ID}"
echo "Public Subnet: ${PUBLIC_SUBNET_ID}"
echo "Private Subnet: ${PRIVATE_SUBNET_ID}"
echo "Security Group: ${SECURITY_GROUP_ID}"
echo "FSx Lustre ID: ${FSX_LUSTRE_ID}"
echo "FSx OpenZFS Volume ID: ${FSX_OPENZFS_ROOT_VOLUME_ID}"
echo "Head Node Bootstrap: ${HEAD_NODE_BOOTSTRAP_SCRIPT}"
echo "Compute Node Bootstrap: ${COMPUTE_NODE_BOOTSTRAP_SCRIPT}"
```

### í´ëŸ¬ìŠ¤í„° ì„¤ì • íŒŒì¼ ìƒì„±

í™˜ê²½ ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ í´ëŸ¬ìŠ¤í„° ì„¤ì • íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤:

> ðŸ“ **í…œí”Œë¦¿ íŒŒì¼ ì°¸ì¡°:**
> - ì „ì²´ ì„¤ì • íŒŒì¼ í…œí”Œë¦¿ì€ [examples/templates/cluster-config.yaml.template](../examples/templates/cluster-config.yaml.template)ì—ì„œ í™•ì¸í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.
> - ì•„ëž˜ ëª…ë ¹ì€ í™˜ê²½ ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ í…œí”Œë¦¿ì„ ì‹¤ì œ ì„¤ì • íŒŒì¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

```bash
# ì„¤ì • íŒŒì¼ ë””ë ‰í† ë¦¬ë¡œ ì´ë™
cd ~/distributed-training-on-aws/pcluster-container
mkdir -p examples/configs

# ì¸ìŠ¤í„´ìŠ¤ íƒ€ìž… ë° ìˆ˜ëŸ‰ ì„¤ì • (í•„ìš”ì‹œ ë³€ê²½)
export COMPUTE_INSTANCE_TYPE=g5.8xlarge
export COMPUTE_MIN_COUNT=2
export COMPUTE_MAX_COUNT=2

# í´ëŸ¬ìŠ¤í„° ì„¤ì • íŒŒì¼ ìƒì„±
cat > examples/configs/cluster-config.yaml << EOF
# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: MIT-0
Region: ${AWS_REGION}

DevSettings:
  Timeouts:
    HeadNodeBootstrapTimeout: 43200  # 12 hours
    ComputeNodeBootstrapTimeout: 7200  # 2 hours

Imds:
  ImdsSupport: v2.0

Image:
  Os: ubuntu2204

HeadNode:
  InstanceType: m5.8xlarge
  Networking:
    SubnetId: ${PUBLIC_SUBNET_ID}
    ElasticIp: false
    AdditionalSecurityGroups:
      - ${SECURITY_GROUP_ID}
  LocalStorage:
    RootVolume:
      Size: 500
      DeleteOnTermination: true  # Root and /home volume for users
  Iam:
    AdditionalIamPolicies:
      # Grant ECR, SSM and S3 access
      - Policy: arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore
      - Policy: arn:aws:iam::aws:policy/AmazonS3FullAccess
      - Policy: arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryFullAccess
      - Policy: arn:aws:iam::aws:policy/AmazonElasticContainerRegistryPublicFullAccess
  CustomActions:
    OnNodeConfigured:
      Sequence:
        - Script: 'https://raw.githubusercontent.com/aws-samples/aws-parallelcluster-post-install-scripts/main/docker/postinstall.sh'
        - Script: '${HEAD_NODE_BOOTSTRAP_SCRIPT}'

Scheduling:
  Scheduler: slurm
  SlurmSettings:
    ScaledownIdletime: -1  # Disable automatic scale-down
    QueueUpdateStrategy: DRAIN
    CustomSlurmSettings:
      # Simple accounting to text file
      - JobCompType: jobcomp/filetxt
      - JobCompLoc: /home/slurm/slurm-job-completions.txt
      - JobAcctGatherType: jobacct_gather/linux
      # Increase timeout before marking node as DOWN
      - SlurmdTimeout: 1800
      - SuspendTimeout: 300
      - ReturnToService: 2
  SlurmQueues:
    - Name: compute-gpu
      CapacityType: ONDEMAND
      # Capacity Reservation ì‚¬ìš© ì‹œ ì•„ëž˜ ì²˜ëŸ¼ CAPACITY_BLOCKìœ¼ë¡œ ìˆ˜ì •
      # CapacityType: CAPACITY_BLOCK
      Networking:
        SubnetIds:
          - ${PRIVATE_SUBNET_ID}
        PlacementGroup:
          Enabled: true  # Capacity Reservation ì‚¬ìš© ì‹œ falseë¡œ ë³€ê²½
        AdditionalSecurityGroups:
          - ${SECURITY_GROUP_ID}
      ComputeSettings:
        LocalStorage:
          EphemeralVolume:
            MountDir: /scratch  # Local NVMe scratch space
          RootVolume:
            Size: 512
      JobExclusiveAllocation: true  # Each job gets exclusive access to nodes
      ComputeResources:
        - Name: distributed-ml
          InstanceType: ${COMPUTE_INSTANCE_TYPE}
          MinCount: ${COMPUTE_MIN_COUNT}
          MaxCount: ${COMPUTE_MAX_COUNT}
          # Capacity Reservation ì‚¬ìš© ì‹œ ì•„ëž˜ ì£¼ì„ í•´ì œ
          # CapacityReservationTarget:
          #   CapacityReservationId: cr-0a1f6b92ded769450  # Replace with your Capacity Reservation ID
          Efa:
            Enabled: true
            #GdrSupport: true  # p4d/p5 ì¸ìŠ¤í„´ìŠ¤ë§Œ ì§€ì›
      Iam:
        AdditionalIamPolicies:
          - Policy: arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore
          - Policy: arn:aws:iam::aws:policy/AmazonS3FullAccess
          - Policy: arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryFullAccess
          - Policy: arn:aws:iam::aws:policy/AmazonElasticContainerRegistryPublicFullAccess
      CustomActions:
        OnNodeConfigured:
          Sequence:
            - Script: 'https://raw.githubusercontent.com/aws-samples/aws-parallelcluster-post-install-scripts/main/docker/postinstall.sh'
            - Script: '${COMPUTE_NODE_BOOTSTRAP_SCRIPT}'

SharedStorage:
  - Name: shared-workspace-zfs
    StorageType: FsxOpenZfs
    MountDir: /fsx
    FsxOpenZfsSettings:
      VolumeId: ${FSX_OPENZFS_ROOT_VOLUME_ID}

  - Name: fsx-lustre
    MountDir: /lustre
    StorageType: FsxLustre
    FsxLustreSettings:
      FileSystemId: ${FSX_LUSTRE_ID}

Monitoring:
  DetailedMonitoring: true
  Logs:
    CloudWatch:
      Enabled: true
  Dashboards:
    CloudWatch:
      Enabled: true
EOF

echo "âœ… Cluster configuration file created: examples/configs/cluster-config.yaml"
```

### ì„¤ì • íŒŒì¼ ì£¼ìš” êµ¬ì„± ìš”ì†Œ

**HeadNode ì„¤ì •:**
- **InstanceType**: `m5.8xlarge` - 32 vCPU, 128GB RAM (Slurm ì»¨íŠ¸ë¡¤ëŸ¬ìš©)
- **SubnetId**: Public ì„œë¸Œë„· ì‚¬ìš© (Session Manager ì ‘ì†ìš©)
- **RootVolume**: 500GB (ì‚¬ìš©ìž í™ˆ ë””ë ‰í† ë¦¬ í¬í•¨)
- **CustomActions**: Docker ë° Enroot/Pyxis ì„¤ì¹˜ ìŠ¤í¬ë¦½íŠ¸

**Compute Node ì„¤ì •:**
- **InstanceType**: í™˜ê²½ ë³€ìˆ˜ë¡œ ì„¤ì • (ê¸°ë³¸: `g5.8xlarge`)
- **SubnetId**: Private ì„œë¸Œë„·
- **MinCount/MaxCount**: í™˜ê²½ ë³€ìˆ˜ë¡œ ì¡°ì • ê°€ëŠ¥
- **EFA**: í™œì„±í™” (ê³ ì„±ëŠ¥ ë„¤íŠ¸ì›Œí‚¹)
- **LocalStorage**: 
  - `/scratch`: NVMe ìž„ì‹œ ìŠ¤í† ë¦¬ì§€
  - Root Volume: 512GB
- **CustomActions**: Docker ë° Enroot/Pyxis ì„¤ì¹˜ ìŠ¤í¬ë¦½íŠ¸

**SharedStorage:**
- **FSx OpenZFS** (`/fsx`): Home ë””ë ‰í† ë¦¬ ë° ì‚¬ìš©ìž ë°ì´í„°
- **FSx Lustre** (`/lustre`): í•™ìŠµ ë°ì´í„°ì…‹
  - ê¸°ì¡´ FSx Lustre íŒŒì¼ ì‹œìŠ¤í…œì„ ì‚¬ìš© (`FileSystemId`ë¡œ ì°¸ì¡°)
  - **Data Repository Association (DRA)**ì€ ì´ë¯¸ 01-prerequisites.mdì—ì„œ ì„¤ì •ë¨
  - í´ëŸ¬ìŠ¤í„°ê°€ ë§ˆìš´íŠ¸í•˜ë©´ ê¸°ì¡´ DRA ì„¤ì •ì´ ìžë™ìœ¼ë¡œ ì ìš©ë¨:
    - `/lustre/data` â†” `s3://${S3_BUCKET_NAME}/data/`
    - `/lustre/checkpoints` â†” `s3://${S3_BUCKET_NAME}/checkpoints/`
    - `/lustre/logs` â†” `s3://${S3_BUCKET_NAME}/logs/`
    - `/lustre/results` â†” `s3://${S3_BUCKET_NAME}/results/`

> ðŸ’¡ **DRAëŠ” FSx ë ˆë²¨ì˜ ì„¤ì •**ì´ë¯€ë¡œ ParallelCluster YAMLì—ì„œ ë³„ë„ ì„¤ì •ì´ í•„ìš” ì—†ìŠµë‹ˆë‹¤.

---

### Capacity Reservation ì‚¬ìš© (ì„ íƒì‚¬í•­)

GPU ì¸ìŠ¤í„´ìŠ¤ ê°€ìš©ì„±ì„ ë³´ìž¥í•˜ê¸° ìœ„í•´ Capacity Reservationì„ ì‚¬ìš©í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.

#### Capacity Reservation ìƒì„± (AWS Console ë˜ëŠ” CLI)

```bash
# Capacity Reservation ìƒì„± ì˜ˆì‹œ
aws ec2 create-capacity-reservation \
  --instance-type g5.12xlarge \
  --instance-platform Linux/UNIX \
  --availability-zone ${PRIMARY_AZ} \
  --instance-count 2 \
  --instance-match-criteria targeted \
  --region ${AWS_REGION}
```

#### ì„¤ì • íŒŒì¼ ìˆ˜ì •

Capacity Reservationì„ ì‚¬ìš©í•˜ë ¤ë©´ ì„¤ì • íŒŒì¼ì—ì„œ ë‹¤ìŒì„ ìˆ˜ì •:

1. **PlacementGroup ë¹„í™œì„±í™”**:
```yaml
PlacementGroup:
  Enabled: false  # Capacity Reservationê³¼ í•¨ê»˜ ì‚¬ìš© ë¶ˆê°€
```

2. **CapacityReservationTarget ì¶”ê°€**:
```yaml
ComputeResources:
  - Name: distributed-ml
    InstanceType: ${COMPUTE_INSTANCE_TYPE}
    MinCount: 2  # Reservation ìˆ˜ëŸ‰ê³¼ ì¼ì¹˜
    MaxCount: 2
    CapacityReservationTarget:
      CapacityReservationId: cr-0123456789abcdef0  # ì‹¤ì œ IDë¡œ ë³€ê²½
```

> âš ï¸ **ì£¼ì˜:** Capacity Reservation ì‚¬ìš© ì‹œ MinCountë¥¼ Reservation ìˆ˜ëŸ‰ì— ë§žì¶° ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤.

---

### ì„¤ì • íŒŒì¼ í™•ì¸

ìƒì„±ëœ ì„¤ì • íŒŒì¼ì„ í™•ì¸í•©ë‹ˆë‹¤:

```bash
# ì„¤ì • íŒŒì¼ ë‚´ìš© í™•ì¸
cat examples/configs/cluster-config.yaml

> âš ï¸ **ì£¼ì˜:**
> - í™˜ê²½ ë³€ìˆ˜ê°€ ì œëŒ€ë¡œ ì¹˜í™˜ë˜ì—ˆëŠ”ì§€ ê¼­ í™•ì¸í•˜ì„¸ìš”.

---

## 2.2 í´ëŸ¬ìŠ¤í„° ìƒì„±

### í´ëŸ¬ìŠ¤í„° ì´ë¦„ ì„¤ì •

```bash
export CLUSTER_NAME=ml-training-cluster
```

### í´ëŸ¬ìŠ¤í„° ìƒì„± ì‹œìž‘

```bash
# í´ëŸ¬ìŠ¤í„° ìƒì„±
pcluster create-cluster \
  --cluster-name ${CLUSTER_NAME} \
  --cluster-configuration examples/configs/cluster-config.yaml \
  --region ${AWS_REGION}
```

**ì˜ˆìƒ ì¶œë ¥:**
```json
{
  "cluster": {
    "clusterName": "ml-training-cluster",
    "cloudformationStackStatus": "CREATE_IN_PROGRESS",
    "cloudformationStackArn": "arn:aws:cloudformation:us-east-1:123456789012:stack/ml-training-cluster/883840a0-cd49-11f0-8ba1-0edd122729eb",
    "region": "us-east-1",
    "version": "3.14.0",
    "clusterStatus": "CREATE_IN_PROGRESS",
    "scheduler": {
      "type": "slurm"
    }
  },
  "validationMessages": [
    {
      "level": "WARNING",
      "type": "DetailedMonitoringValidator",
      "message": "Detailed Monitoring is enabled for EC2 instances in your compute fleet. The Amazon EC2 console will display monitoring graphs with a 1-minute period for these instances. Note that this will increase the cost. If you want to avoid this and use basic monitoring instead, please set `Monitoring / DetailedMonitoring` to false."
    },
    {
      "level": "WARNING",
      "type": "KeyPairValidator",
      "message": "If you do not specify a key pair, you can't connect to the instance unless you choose an AMI that is configured to allow users another way to log in"
    }
  ]
}
```

### í´ëŸ¬ìŠ¤í„° ìƒì„± ìƒíƒœ ëª¨ë‹ˆí„°ë§

```bash
# í´ëŸ¬ìŠ¤í„° ìƒíƒœ í™•ì¸
pcluster describe-cluster \
  --cluster-name ${CLUSTER_NAME} \
  --region ${AWS_REGION} \
  --query 'clusterStatus'
```

**ì˜ˆìƒ ìƒíƒœ:**
- `CREATE_IN_PROGRESS`: ìƒì„± ì¤‘
- `CREATE_COMPLETE`: ìƒì„± ì™„ë£Œ âœ…
- `CREATE_FAILED`: ìƒì„± ì‹¤íŒ¨ âŒ

> â±ï¸ **ì˜ˆìƒ ì†Œìš” ì‹œê°„: ì•½ 25-35ë¶„**

### CloudFormation ìŠ¤íƒ í™•ì¸

```bash
# CloudFormation ìŠ¤íƒ ì´ë²¤íŠ¸ í™•ì¸
aws cloudformation describe-stack-events \
  --stack-name ${CLUSTER_NAME} \
  --region ${AWS_REGION} \
  --max-items 10 \
  --query 'StackEvents[*].[Timestamp,ResourceStatus,ResourceType,LogicalResourceId]' \
  --output table
```

---

## 2.3 í´ëŸ¬ìŠ¤í„° ì ‘ì† ë° ê²€ì¦

í´ëŸ¬ìŠ¤í„° ìƒì„±ì´ ì™„ë£Œë˜ë©´ Head Nodeì— ì ‘ì†í•˜ì—¬ í™˜ê²½ì„ í™•ì¸í•©ë‹ˆë‹¤.

### Head Node ì ‘ì†

Session Managerë¥¼ í†µí•´ Head Nodeì— ì ‘ì†í•©ë‹ˆë‹¤:

```bash
aws ssm start-session --target $(pcluster describe-cluster --region ${AWS_REGION} -n ${CLUSTER_NAME} | jq '.headNode.instanceId' | tr -d '"')
```

**ì˜ˆìƒ ì¶œë ¥:**
```
$ aws ssm start-session --target $(pcluster describe-cluster --region ${AWS_REGION} -n ${CLUSTER_NAME} | jq '.headNode.instanceId' | tr -d '"')

Starting session with SessionId: i-06e9f603643fc3f26-alsudcf75qe2lzy5vdidhs825i
$ 
```

Ubuntu ì‚¬ìš©ìžë¡œ ì „í™˜ í•©ë‹ˆë‹¤.
```bash
sudo su - ubuntu
```

> ðŸ’¡ **Session Manager ì‚¬ìš©:**
> - SSH í‚¤ ì—†ì´ ì•ˆì „í•˜ê²Œ ì ‘ì†
> - IAM ê¸°ë°˜ ì¸ì¦
> - ì„¸ì…˜ ë¡œê·¸ ìžë™ ê¸°ë¡

---

### âš ï¸ FSx Lustre ë§ˆìš´íŠ¸ ë¬¸ì œ í•´ê²° (ì¤‘ìš”)

> ðŸš¨ **í˜„ìž¬ ì´ìŠˆ (2025.11.30 ê¸°ì¤€):**
> 
> Head Node / Compute Node ì˜ Lustre í´ë¼ì´ì–¸íŠ¸ ì»¤ë„ ëª¨ë“ˆ ë²„ì „ê³¼ ì‹¤ì œ ì‹œìŠ¤í…œ ì»¤ë„ ë²„ì „ì´ ì¼ì¹˜í•˜ì§€ ì•Šì•„ 
> FSx Lustreê°€ ìžë™ìœ¼ë¡œ ë§ˆìš´íŠ¸ë˜ì§€ ì•ŠëŠ” í˜„ìƒì´ ë°œìƒí•˜ê³  ìžˆìŠµë‹ˆë‹¤.
> 
> ì´ëŠ” Ubuntu 22.04 ì´ë¯¸ì§€ì˜ ì»¤ë„ ì—…ë°ì´íŠ¸ì™€ Lustre í´ë¼ì´ì–¸íŠ¸ íŒ¨í‚¤ì§€ ë²„ì „ ë¶ˆì¼ì¹˜ë¡œ ì¸í•œ ë¬¸ì œìž…ë‹ˆë‹¤.
> **í˜„ìž¬ëŠ” ìˆ˜ë™ìœ¼ë¡œ ë§ˆìš´íŠ¸ë¥¼ ì§„í–‰í•´ì•¼ í•˜ë©°, í–¥í›„ ì—…ë°ì´íŠ¸ ì‹œ ìžë™í™” ë°©ë²•ì„ ì•ˆë‚´í•˜ê² ìŠµë‹ˆë‹¤.**

#### 1. ë§ˆìš´íŠ¸ ìƒíƒœ í™•ì¸

ë¨¼ì € FSx Lustreê°€ ì •ìƒì ìœ¼ë¡œ ë§ˆìš´íŠ¸ë˜ì—ˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤:

```bash
# ë§ˆìš´íŠ¸ëœ íŒŒì¼ì‹œìŠ¤í…œ í™•ì¸
df -h | grep lustre

# ë˜ëŠ” ì§ì ‘ ë””ë ‰í† ë¦¬ ì ‘ê·¼ ì‹œë„
ls -la /lustre
```

**ì •ìƒì ì¸ ê²½ìš° (ìžë™ ë§ˆìš´íŠ¸ ì„±ê³µ):**
```
10.1.30.23@tcp:/czrc3amv  2.2T   16M  2.2T   1% /lustre
```

**ë¬¸ì œê°€ ìžˆëŠ” ê²½ìš°:**
```bash
ls: cannot open directory '/lustre': No such device
```

#### 2. ì»¤ë„ ë° Lustre ë²„ì „ ë¶ˆì¼ì¹˜ í™•ì¸

ë§ˆìš´íŠ¸ê°€ ì‹¤íŒ¨í•œ ê²½ìš°, ì»¤ë„ ë²„ì „ê³¼ Lustre ëª¨ë“ˆ ë²„ì „ì„ í™•ì¸í•©ë‹ˆë‹¤:

```bash
# í˜„ìž¬ ì‹¤í–‰ ì¤‘ì¸ ì»¤ë„ ë²„ì „ í™•ì¸
uname -r

# ì„¤ì¹˜ëœ Lustre í´ë¼ì´ì–¸íŠ¸ ëª¨ë“ˆ í™•ì¸
dpkg -l | grep lustre
```

**ì˜ˆìƒ ì¶œë ¥:**
```bash
# uname -r
6.8.0-1043-aws

# dpkg -l | grep lustre
ii  lustre-client-modules-6.8.0-1039-aws  2.15.6-1fsx21  amd64
```

> ðŸ“ **ë¬¸ì œ ì›ì¸:** 
> - ì‹œìŠ¤í…œ ì»¤ë„: `6.8.0-1043-aws`
> - Lustre ëª¨ë“ˆ: `6.8.0-1039-aws`
> - **ë²„ì „ ë¶ˆì¼ì¹˜**ë¡œ ì¸í•´ Lustre ëª¨ë“ˆì„ ë¡œë“œí•  ìˆ˜ ì—†ìŒ

#### 3. ì˜¬ë°”ë¥¸ Lustre ëª¨ë“ˆ ì„¤ì¹˜

í˜„ìž¬ ì»¤ë„ ë²„ì „ì— ë§žëŠ” Lustre í´ë¼ì´ì–¸íŠ¸ ëª¨ë“ˆì„ ì„¤ì¹˜í•©ë‹ˆë‹¤:

```bash
# rpm íŒ¨í‚¤ì§€ ê´€ë¦¬ ë„êµ¬ ì„¤ì¹˜ (í•„ìš” ì‹œ)
sudo apt-get install -y rpm

# í˜„ìž¬ ì»¤ë„ ë²„ì „ì— ë§žëŠ” Lustre ëª¨ë“ˆ ì„¤ì¹˜
sudo apt-get install -y lustre-client-modules-$(uname -r)
```

**ì˜ˆìƒ ì¶œë ¥:**
```
Reading package lists... Done
Building dependency tree... Done
Reading state information... Done
The following NEW packages will be installed:
  lustre-client-modules-6.8.0-1043-aws
0 upgraded, 1 newly installed, 0 to remove and 32 not upgraded.
Need to get 25.2 MB of archives.
After this operation, 128 MB of additional disk space will be used.
Get:1 https://fsx-lustre-client-repo.s3.amazonaws.com/ubuntu jammy/main amd64 lustre-client-modules-6.8.0-1043-aws amd64 2.15.6-1fsx25 [25.2 MB]
Fetched 25.2 MB in 0s (68.9 MB/s)
Selecting previously unselected package lustre-client-modules-6.8.0-1043-aws.
...
Setting up lustre-client-modules-6.8.0-1043-aws (2.15.6-1fsx25) ...
```

#### 4. Lustre ì»¤ë„ ëª¨ë“ˆ ë¡œë“œ

Lustre íŒŒì¼ì‹œìŠ¤í…œ ëª¨ë“ˆì„ ì»¤ë„ì— ë¡œë“œí•©ë‹ˆë‹¤:

```bash
# Lustre ëª¨ë“ˆ ë¡œë“œ
sudo modprobe lustre

# ëª¨ë“ˆ ë¡œë“œ í™•ì¸
lsmod | grep lustre
```

**ì˜ˆìƒ ì¶œë ¥:**
```
lustre               1126400  0
mdc                   294912  1 lustre
lov                   356352  2 mdc,lustre
lmv                   229376  1 lustre
ptlrpc               1544192  7 fld,osc,fid,lov,mdc,lmv,lustre
obdclass             3399680  8 fld,osc,fid,ptlrpc,lov,mdc,lmv,lustre
lnet                  839680  6 osc,obdclass,ptlrpc,ksocklnd,lmv,lustre
libcfs                237568  11 fld,lnet,osc,fid,obdclass,ptlrpc,ksocklnd,lov,mdc,lmv,lustre
```

#### 5. íŒŒì¼ì‹œìŠ¤í…œ ë“±ë¡ í™•ì¸

Lustre íŒŒì¼ì‹œìŠ¤í…œì´ ì»¤ë„ì— ë“±ë¡ë˜ì—ˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤:

```bash
# ì§€ì›ë˜ëŠ” íŒŒì¼ì‹œìŠ¤í…œ í™•ì¸
cat /proc/filesystems | grep lustre
```

**ì˜ˆìƒ ì¶œë ¥:**
```
nodev   lustre
```

âœ… `lustre`ê°€ í‘œì‹œë˜ë©´ ì •ìƒìž…ë‹ˆë‹¤!

#### 6. FSx Lustre ë§ˆìš´íŠ¸

ì´ì œ FSx Lustreë¥¼ ìˆ˜ë™ìœ¼ë¡œ ë§ˆìš´íŠ¸í•©ë‹ˆë‹¤:

```bash
# Lustre ë§ˆìš´íŠ¸
sudo mount /lustre

# ë§ˆìš´íŠ¸ í™•ì¸
df -h | grep lustre
```

**ì˜ˆìƒ ì¶œë ¥:**
```
10.1.30.23@tcp:/czrc3amv  2.2T   16M  2.2T   1% /lustre
```

#### 7. Lustre ë””ë ‰í† ë¦¬ êµ¬ì¡° í™•ì¸

ë§ˆìš´íŠ¸ê°€ ì„±ê³µí•˜ë©´ DRAë¡œ ì—°ê²°ëœ ë””ë ‰í† ë¦¬ë¥¼ í™•ì¸í•©ë‹ˆë‹¤:

```bash
# Lustre ë””ë ‰í† ë¦¬ í™•ì¸
ls -la /lustre/
```

**ì˜ˆìƒ ì¶œë ¥:**
```
total 167
drwxrwxrwt  8 root root 33280 Nov 29 17:27 .
drwxr-xr-x 23 root root  4096 Nov 29 17:50 ..
drwxrwxr-x  2 root root 33280 Nov 29 17:03 checkpoints
drwxrwxr-x  3 root root 33280 Nov 29 16:17 data
drwxrwxr-x  2 root root 33280 Nov 29 17:24 logs
drwxrwxr-x  2 root root 33280 Nov 29 17:27 results
```

âœ… **ë§ˆìš´íŠ¸ ì„±ê³µ!** ì´ì œ ì •ìƒì ìœ¼ë¡œ FSx Lustreë¥¼ ì‚¬ìš©í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.

---

### ê¸°ë³¸ í™˜ê²½ í™•ì¸

Head Nodeì— ì ‘ì†í•œ í›„ ë‹¤ìŒ ëª…ë ¹ìœ¼ë¡œ í™˜ê²½ì„ í™•ì¸í•©ë‹ˆë‹¤:

#### OS ì •ë³´

```bash
# OS ì •ë³´
cat /etc/os-release
```

**ì˜ˆìƒ ì¶œë ¥:**
```
NAME="Ubuntu"
VERSION="22.04.x LTS (Jammy Jellyfish)"
ID=ubuntu
ID_LIKE=debian

...

```

#### ê³µìœ  ìŠ¤í† ë¦¬ì§€ í™•ì¸

```bash
# ë§ˆìš´íŠ¸ëœ ê³µìœ  ìŠ¤í† ë¦¬ì§€ í™•ì¸
df -h | grep -E 'fsx|lustre'

# ë˜ëŠ” ì „ì²´ ë§ˆìš´íŠ¸ í™•ì¸
mount | grep -E 'fsx|lustre'
```

**ì˜ˆìƒ ì¶œë ¥:**
```
10.0.1.100@tcp:/fsvol-xxx  512G   64M  512G   1% /fsx
10.0.1.101@tcp:/yyyyyyy    1.2T  1.1M  1.2T   1% /lustre
```

#### WikiText-2 ë°ì´í„°ì…‹ í™•ì¸

```bash
# 01-prerequisites.mdì—ì„œ ì—…ë¡œë“œí•œ ë°ì´í„° í™•ì¸
ls -lh /lustre/data/wikitext-2/
```

**ì˜ˆìƒ ì¶œë ¥:**
```
total 0
-rw-r--r-- 1 root root   43 Nov 29 16:49 dataset_dict.json
drwxr-xr-x 2 root root 4.0K Nov 29 16:49 test
drwxr-xr-x 2 root root 4.0K Nov 29 16:49 train
drwxr-xr-x 2 root root 4.0K Nov 29 16:49 validation
```

> ðŸ’¡ **Lazy Loading:** íŒŒì¼ ë©”íƒ€ë°ì´í„°ëŠ” ì¦‰ì‹œ ë³´ì´ì§€ë§Œ, ì‹¤ì œ ë°ì´í„°ëŠ” íŒŒì¼ ì ‘ê·¼ ì‹œ S3ì—ì„œ ë¡œë“œë©ë‹ˆë‹¤.

#### Enroot í™•ì¸

```bash
# Enroot ë²„ì „ í™•ì¸
sudo enroot version
```

**ì˜ˆìƒ ì¶œë ¥:**
```
3.4.1
```

---

## 2.4 Slurm ê¸°ë³¸ ì‚¬ìš©

### Slurm ë…¸ë“œ ìƒíƒœ í™•ì¸

```bash
# ë…¸ë“œ ì •ë³´ í™•ì¸
sinfo
```

**ì˜ˆìƒ ì¶œë ¥:**
```
PARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST
compute-gpu*    up   infinite      2   idle compute-gpu-st-distributed-ml-[1-2]
```

> ðŸ“ **ë…¸ë“œ ìƒíƒœ:**
> - `idle~`: ìœ íœ´ ìƒíƒœ, í•„ìš” ì‹œ ìžë™ í”„ë¡œë¹„ì €ë‹
> - `alloc`: ìž‘ì—…ì— í• ë‹¹ë¨
> - `mix`: ì¼ë¶€ ë¦¬ì†ŒìŠ¤ ì‚¬ìš© ì¤‘
> - `down`: ì‚¬ìš© ë¶ˆê°€

### Slurm íŒŒí‹°ì…˜ í™•ì¸

```bash
# íŒŒí‹°ì…˜ ì •ë³´
scontrol show partition compute-gpu
```

### ê°„ë‹¨í•œ nvidia-smi í…ŒìŠ¤íŠ¸

Compute Nodeë¥¼ í”„ë¡œë¹„ì €ë‹í•˜ê³  GPUë¥¼ í™•ì¸í•˜ëŠ” ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤:

```bash
# nvidia-smi í…ŒìŠ¤íŠ¸ ìž‘ì—… ì œì¶œ
srun --partition=compute-gpu \
     --nodes=1 \
     --ntasks=1 \
     --gpus-per-node=1 \
     nvidia-smi
```

**ì˜ˆìƒ ì¶œë ¥:**
```
ubuntu@ip-10-0-3-12:~$ srun --partition=compute-gpu \
>      --nodes=1 \
>      --ntasks=1 \
>      --gpus-per-node=1 \
>      nvidia-smi
Sat Nov 29 19:21:33 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.172.08             Driver Version: 570.172.08     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A10G                    On  |   00000000:00:1E.0 Off |                    0 |
|  0%   24C    P8             13W /  300W |       0MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
```

### ìž‘ì—… í í™•ì¸

```bash
# ì‹¤í–‰ ì¤‘ì¸ ìž‘ì—… í™•ì¸
squeue

# ë³¸ì¸ì˜ ìž‘ì—…ë§Œ í™•ì¸
squeue -u $USER

# ìž‘ì—… ìƒì„¸ ì •ë³´
scontrol show job 1
```

### Compute Node ìƒíƒœ í™•ì¸

```bash
# í”„ë¡œë¹„ì €ë‹ëœ ë…¸ë“œ í™•ì¸
sinfo

# íŠ¹ì • ë…¸ë“œ ìƒì„¸ ì •ë³´
scontrol show node compute-gpu-st-distributed-ml-1
```

---

## ë‹¤ìŒ ë‹¨ê³„

âœ… ParallelCluster ë°°í¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!

ì´ì œ **[3. ë¶„ì‚° í•™ìŠµ ì‹¤í–‰](./03-distributed-training.md)**ìœ¼ë¡œ ì§„í–‰í•˜ì—¬ ì‹¤ì œ í•™ìŠµ ìž‘ì—…ì„ ì‹¤í–‰í•˜ì„¸ìš”.

---

### í™˜ê²½ ë³€ìˆ˜ ì €ìž¥

```bash
# í´ëŸ¬ìŠ¤í„° ì´ë¦„ ì €ìž¥
cat >> ~/pcluster-env.sh << EOF
export CLUSTER_NAME=${CLUSTER_NAME}
EOF
```

---

## ðŸ”§ ëª¨ë“  ì»´í“¨íŠ¸ ë…¸ë“œì— FSx Lustre ë§ˆìš´íŠ¸í•˜ê¸°

### âš ï¸ FSx Lustre ë§ˆìš´íŠ¸ ë¬¸ì œ í•´ê²° (ì»´í“¨íŠ¸ ë…¸ë“œ)

> ðŸš¨ **í˜„ìž¬ ì•Œë ¤ì§„ ì´ìŠˆ:**
> 
> ì»´í“¨íŠ¸ ë…¸ë“œì—ì„œë„ í—¤ë“œ ë…¸ë“œì™€ ë™ì¼í•˜ê²Œ Lustre í´ë¼ì´ì–¸íŠ¸ ì»¤ë„ ëª¨ë“ˆ ë²„ì „ ë¶ˆì¼ì¹˜ë¡œ ì¸í•´
> FSx Lustreê°€ ìžë™ìœ¼ë¡œ ë§ˆìš´íŠ¸ë˜ì§€ ì•ŠëŠ” í˜„ìƒì´ ë°œìƒí•©ë‹ˆë‹¤.
> 
> ì•„ëž˜ ê°€ì´ë“œë¥¼ ë”°ë¼ **ëª¨ë“  ì»´í“¨íŠ¸ ë…¸ë“œì— í•œ ë²ˆì— ë§ˆìš´íŠ¸**ë¥¼ ì§„í–‰í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.

---

### 1ï¸âƒ£ ë§ˆìš´íŠ¸ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±

í—¤ë“œ ë…¸ë“œì—ì„œ ë‹¤ìŒ ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•˜ì—¬ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤:

```bash
cat > mount_lustre.sh << 'EOF'
#!/bin/bash

# FSx Lustre ë§ˆìš´íŠ¸ ìŠ¤í¬ë¦½íŠ¸
# ëª¨ë“  ì»´í“¨íŠ¸ ë…¸ë“œì—ì„œ ì‹¤í–‰ë  ìŠ¤í¬ë¦½íŠ¸

set -e  # ì—ëŸ¬ ë°œìƒ ì‹œ ì¤‘ë‹¨

NODE_NAME=$(hostname)
TIMESTAMP=$(date '+%Y-%m-%d %H:%M:%S')

echo "[$TIMESTAMP] ðŸ–¥ï¸  Node: $NODE_NAME - Starting FSx Lustre mount process..."

# 1. ì»¤ë„ ë²„ì „ í™•ì¸
KERNEL_VERSION=$(uname -r)
echo "[$TIMESTAMP] ðŸ” Current kernel version: $KERNEL_VERSION"

# 2. ì„¤ì¹˜ëœ Lustre ëª¨ë“ˆ í™•ì¸
echo "[$TIMESTAMP] ðŸ” Checking installed Lustre modules..."
INSTALLED_LUSTRE=$(dpkg -l | grep "lustre-client-modules-$KERNEL_VERSION" || echo "not_found")

if [[ "$INSTALLED_LUSTRE" == "not_found" ]]; then
    echo "[$TIMESTAMP] ðŸ“¦ Installing Lustre client module for kernel $KERNEL_VERSION..."
    
    # rpm íŒ¨í‚¤ì§€ ë„êµ¬ ì„¤ì¹˜ (í•„ìš” ì‹œ)
    if ! command -v rpm &> /dev/null; then
        echo "[$TIMESTAMP] ðŸ“¦ Installing rpm..."
        sudo apt-get update -qq
        sudo apt-get install -y rpm
    fi
    
    # Lustre í´ë¼ì´ì–¸íŠ¸ ëª¨ë“ˆ ì„¤ì¹˜
    sudo apt-get install -y lustre-client-modules-$KERNEL_VERSION
    
    if [ $? -ne 0 ]; then
        echo "[$TIMESTAMP] âŒ Failed to install Lustre module"
        exit 1
    fi
    echo "[$TIMESTAMP] âœ… Lustre module installed successfully"
else
    echo "[$TIMESTAMP] âœ… Lustre module already installed"
fi

# 3. Lustre ì»¤ë„ ëª¨ë“ˆ ë¡œë“œ
echo "[$TIMESTAMP] ðŸ”§ Loading Lustre kernel module..."
if lsmod | grep -q lustre; then
    echo "[$TIMESTAMP] âœ… Lustre module already loaded"
else
    sudo modprobe lustre
    if [ $? -eq 0 ]; then
        echo "[$TIMESTAMP] âœ… Lustre module loaded successfully"
    else
        echo "[$TIMESTAMP] âŒ Failed to load Lustre module"
        exit 1
    fi
fi

# 4. íŒŒì¼ì‹œìŠ¤í…œ ë“±ë¡ í™•ì¸
echo "[$TIMESTAMP] ðŸ” Verifying Lustre filesystem registration..."
if cat /proc/filesystems | grep -q lustre; then
    echo "[$TIMESTAMP] âœ… Lustre filesystem registered"
else
    echo "[$TIMESTAMP] âŒ Lustre filesystem not registered"
    exit 1
fi

# 5. ë§ˆìš´íŠ¸ ë””ë ‰í† ë¦¬ í™•ì¸
if [ ! -d "/lustre" ]; then
    echo "[$TIMESTAMP] ðŸ“ Creating /lustre directory..."
    sudo mkdir -p /lustre
fi

# 6. FSx Lustre ë§ˆìš´íŠ¸
echo "[$TIMESTAMP] ðŸ”§ Mounting FSx Lustre..."
sudo mount /lustre

if [ $? -eq 0 ]; then
    echo "[$TIMESTAMP] âœ… FSx Lustre mounted successfully on $NODE_NAME"
    df -h | grep lustre
else
    echo "[$TIMESTAMP] âŒ Failed to mount FSx Lustre"
    exit 1
fi

echo "[$TIMESTAMP] ðŸŽ‰ Mount process completed on $NODE_NAME"
EOF

chmod +x mount_lustre.sh
```

---

### 2ï¸âƒ£ ë§ˆìš´íŠ¸ ìƒíƒœ í™•ì¸ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±

```bash
cat > check_lustre_mount.sh << 'EOF'
#!/bin/bash

# Lustre ë§ˆìš´íŠ¸ ìƒíƒœ í™•ì¸ ìŠ¤í¬ë¦½íŠ¸

NODE_NAME=$(hostname)

echo "Node: $NODE_NAME"
if mountpoint -q /lustre; then
    echo "  Status: âœ… MOUNTED"
    df -h | grep lustre | awk '{print "  Size: " $2 ", Used: " $3 ", Available: " $4 ", Usage: " $5}'
else
    echo "  Status: âŒ NOT MOUNTED"
fi
EOF

chmod +x check_lustre_mount.sh
```

---

### 3ï¸âƒ£ ëª¨ë“  ì»´í“¨íŠ¸ ë…¸ë“œì— ë§ˆìš´íŠ¸ ì‹¤í–‰

#### `srun`ìœ¼ë¡œ ì¦‰ì‹œ ì‹¤í–‰

```bash
# ëª¨ë“  ë…¸ë“œì—ì„œ ë™ì‹œ ì‹¤í–‰
srun --nodes=2 ./mount_lustre.sh
```

**ì˜ˆìƒ ì¶œë ¥:**
```
[2025-11-29 20:42:48] ðŸ–¥ï¸  Node: compute-dy-g5-1 - Starting FSx Lustre mount process...
[2025-11-29 20:42:48] ðŸ–¥ï¸  Node: compute-dy-g5-2 - Starting FSx Lustre mount process...
[2025-11-29 20:42:48] ðŸ“ Checking if Lustre is already mounted...
[2025-11-29 20:42:48] âš ï¸  Lustre not mounted. Proceeding with mount process...
[2025-11-29 20:42:48] ðŸ” Current kernel version: 6.8.0-1043-aws
[2025-11-29 20:42:48] ðŸ“¦ Installing Lustre client module for kernel 6.8.0-1043-aws...
...
[2025-11-29 20:42:48] âœ… Lustre module loaded successfully
[2025-11-29 20:42:48] ðŸ” Verifying Lustre filesystem registration...
[2025-11-29 20:42:48] âœ… Lustre filesystem registered
[2025-11-29 20:42:48] ðŸ”§ Mounting FSx Lustre...
[2025-11-29 20:42:48] âœ… FSx Lustre mounted successfully on compute-gpu-st-distributed-ml-2
10.1.30.23@tcp:/czrc3amv                               2.2T   16M  2.2T   1% /lustre
[2025-11-29 20:42:48] ðŸŽ‰ Mount process completed on compute-gpu-st-distributed-ml-2
[2025-11-29 20:42:48] âœ… Lustre module installed successfully
[2025-11-29 20:42:48] ðŸ”§ Loading Lustre kernel module...
[2025-11-29 20:42:48] âœ… Lustre module loaded successfully
[2025-11-29 20:42:48] ðŸ” Verifying Lustre filesystem registration...
[2025-11-29 20:42:48] âœ… Lustre filesystem registered
[2025-11-29 20:42:48] ðŸ”§ Mounting FSx Lustre...
[2025-11-29 20:42:48] âœ… FSx Lustre mounted successfully on compute-gpu-st-distributed-ml-1
10.1.30.23@tcp:/czrc3amv                               2.2T   16M  2.2T   1% /lustre
[2025-11-29 20:42:48] ðŸŽ‰ Mount process completed on compute-gpu-st-distributed-ml-1
```

---

### 4ï¸âƒ£ ë§ˆìš´íŠ¸ ìƒíƒœ í™•ì¸

```bash
# ëª¨ë“  ë…¸ë“œì—ì„œ ë§ˆìš´íŠ¸ ìƒíƒœ í™•ì¸
srun --nodes=2 ./check_lustre_mount.sh
```

**ì˜ˆìƒ ì¶œë ¥:**
```
Node: compute-dy-g5-1
  Status: âœ… MOUNTED
  Size: 2.2T, Used: 16M, Available: 2.2T, Usage: 1%
Node: compute-dy-g5-2
  Status: âœ… MOUNTED
  Size: 2.2T, Used: 16M, Available: 2.2T, Usage: 1%
```

---

## ðŸ“š ë„¤ë¹„ê²Œì´ì…˜

| ì´ì „ | ìƒìœ„ | ë‹¤ìŒ |
|------|------|------|
| [â—€ ì‚¬ì „ ìš”êµ¬ì‚¬í•­](./01-prerequisites.md) | [ðŸ“‘ ëª©ì°¨](../README.md#-ê°€ì´ë“œ-ëª©ì°¨) | [ë¶„ì‚° í•™ìŠµ â–¶](./03-distributed-training.md) |