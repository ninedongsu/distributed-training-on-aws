## 2. ParallelCluster ë°°í¬

> ğŸ’¡ **ëª©í‘œ:** AWS ParallelClusterë¥¼ ìƒì„±í•˜ê³  Slurm ìŠ¤ì¼€ì¤„ëŸ¬ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.

â±ï¸ **ì˜ˆìƒ ì†Œìš” ì‹œê°„:** 30-40ë¶„

## ëª©ì°¨

- [ê°œìš”](#ê°œìš”)
- [2.1 í´ëŸ¬ìŠ¤í„° ì„¤ì • íŒŒì¼ ì¤€ë¹„](#21-í´ëŸ¬ìŠ¤í„°-ì„¤ì •-íŒŒì¼-ì¤€ë¹„)
- [2.2 í´ëŸ¬ìŠ¤í„° ìƒì„±](#22-í´ëŸ¬ìŠ¤í„°-ìƒì„±)
- [2.3 í´ëŸ¬ìŠ¤í„° ì ‘ì† ë° ê²€ì¦](#23-í´ëŸ¬ìŠ¤í„°-ì ‘ì†-ë°-ê²€ì¦)
- [2.4 Slurm ê¸°ë³¸ ì‚¬ìš©](#24-slurm-ê¸°ë³¸-ì‚¬ìš©)
- [ë‹¤ìŒ ë‹¨ê³„](#ë‹¤ìŒ-ë‹¨ê³„)

---

## ê°œìš”

ì´ ë¬¸ì„œì—ì„œëŠ” ë‹¤ìŒ ì‘ì—…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤:

- âœ… ParallelCluster ì„¤ì • íŒŒì¼ ì‘ì„±
- âœ… í´ëŸ¬ìŠ¤í„° ìƒì„± ë° ê²€ì¦
- âœ… Slurm ê¸°ë³¸ ëª…ë ¹ì–´ ì‚¬ìš© (nvidia-smi w/ srun)

---

## 2.1 í´ëŸ¬ìŠ¤í„° ì„¤ì • íŒŒì¼ ì¤€ë¹„

ParallelCluster ìƒì„±ì„ ìœ„í•œ YAML ì„¤ì • íŒŒì¼ì„ ì¤€ë¹„í•©ë‹ˆë‹¤.

### í™˜ê²½ ë³€ìˆ˜ í™•ì¸

ë¨¼ì € ì´ì „ ë‹¨ê³„ì—ì„œ ì„¤ì •í•œ í™˜ê²½ ë³€ìˆ˜ë“¤ì´ ë¡œë“œë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤:

```bash
# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
source ~/pcluster-env.sh

# ì£¼ìš” ë³€ìˆ˜ í™•ì¸
echo "Region: ${AWS_REGION}"
echo "VPC ID: ${VPC_ID}"
echo "Public Subnet: ${PUBLIC_SUBNET_ID}"
echo "Private Subnet: ${PRIVATE_SUBNET_ID}"
echo "Security Group: ${SECURITY_GROUP_ID}"
echo "FSx Lustre ID: ${FSX_LUSTRE_ID}"
echo "FSx OpenZFS Volume ID: ${FSX_OPENZFS_ROOT_VOLUME_ID}"
echo "Head Node Bootstrap: ${HEAD_NODE_BOOTSTRAP_SCRIPT}"
echo "Compute Node Bootstrap: ${COMPUTE_NODE_BOOTSTRAP_SCRIPT}"
```

### í´ëŸ¬ìŠ¤í„° ì„¤ì • íŒŒì¼ ìƒì„±

í™˜ê²½ ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ í´ëŸ¬ìŠ¤í„° ì„¤ì • íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤:

> ğŸ“ **í…œí”Œë¦¿ íŒŒì¼ ì°¸ì¡°:**
> - ì „ì²´ ì„¤ì • íŒŒì¼ í…œí”Œë¦¿ì€ [examples/templates/cluster-config.yaml.template](../examples/templates/cluster-config.yaml.template)ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
> - ì•„ë˜ ëª…ë ¹ì€ í™˜ê²½ ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ í…œí”Œë¦¿ì„ ì‹¤ì œ ì„¤ì • íŒŒì¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

```bash
# ì„¤ì • íŒŒì¼ ë””ë ‰í† ë¦¬ë¡œ ì´ë™
cd ~/distributed-training-on-aws/pcluster-container

# ì¸ìŠ¤í„´ìŠ¤ íƒ€ì… ë° ìˆ˜ëŸ‰ ì„¤ì • (í•„ìš”ì‹œ ë³€ê²½)
export COMPUTE_INSTANCE_TYPE=g5.8xlarge
export COMPUTE_MIN_COUNT=2
export COMPUTE_MAX_COUNT=2

# í´ëŸ¬ìŠ¤í„° ì„¤ì • íŒŒì¼ ìƒì„±
cat > examples/configs/cluster-config.yaml << EOF
# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: MIT-0
Region: ${AWS_REGION}

DevSettings:
  Timeouts:
    HeadNodeBootstrapTimeout: 43200  # 12 hours
    ComputeNodeBootstrapTimeout: 7200  # 2 hours

Imds:
  ImdsSupport: v2.0

Image:
  Os: ubuntu2204

HeadNode:
  InstanceType: m5.8xlarge
  Networking:
    SubnetId: ${PUBLIC_SUBNET_ID}
    ElasticIp: false
    AdditionalSecurityGroups:
      - ${SECURITY_GROUP_ID}
  LocalStorage:
    RootVolume:
      Size: 500
      DeleteOnTermination: true  # Root and /home volume for users
  Iam:
    AdditionalIamPolicies:
      # Grant ECR, SSM and S3 access
      - Policy: arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore
      - Policy: arn:aws:iam::aws:policy/AmazonS3FullAccess
      - Policy: arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryFullAccess
      - Policy: arn:aws:iam::aws:policy/AmazonElasticContainerRegistryPublicFullAccess
  CustomActions:
    OnNodeConfigured:
      Sequence:
        - Script: 'https://raw.githubusercontent.com/aws-samples/aws-parallelcluster-post-install-scripts/main/docker/postinstall.sh'
        - Script: '${HEAD_NODE_BOOTSTRAP_SCRIPT}'

Scheduling:
  Scheduler: slurm
  SlurmSettings:
    ScaledownIdletime: -1  # Disable automatic scale-down
    QueueUpdateStrategy: DRAIN
    CustomSlurmSettings:
      # Simple accounting to text file
      - JobCompType: jobcomp/filetxt
      - JobCompLoc: /home/slurm/slurm-job-completions.txt
      - JobAcctGatherType: jobacct_gather/linux
      # Increase timeout before marking node as DOWN
      - SlurmdTimeout: 1000
  SlurmQueues:
    - Name: compute-gpu
      CapacityType: ONDEMAND
      Networking:
        SubnetIds:
          - ${PRIVATE_SUBNET_ID}
        PlacementGroup:
          Enabled: true
        AdditionalSecurityGroups:
          - ${SECURITY_GROUP_ID}
      ComputeSettings:
        LocalStorage:
          EphemeralVolume:
            MountDir: /scratch  # Local NVMe scratch space
          RootVolume:
            Size: 512
      JobExclusiveAllocation: true  # Each job gets exclusive access to nodes
      ComputeResources:
        - Name: distributed-ml
          InstanceType: ${COMPUTE_INSTANCE_TYPE}
          MinCount: ${COMPUTE_MIN_COUNT}
          MaxCount: ${COMPUTE_MAX_COUNT}
          # Capacity Reservation configuration (uncomment if using Capacity Block/Reservation)
          # CapacityReservationTarget:
          #   CapacityReservationId: cr-0a1f6b92ded769450  # Replace with your Capacity Reservation ID
          Efa:
            Enabled: true
            #GdrSupport: true  # GPUDirect RDMA for p4d/p5 instances
      Iam:
        AdditionalIamPolicies:
          - Policy: arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore
          - Policy: arn:aws:iam::aws:policy/AmazonS3FullAccess
          - Policy: arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryFullAccess
          - Policy: arn:aws:iam::aws:policy/AmazonElasticContainerRegistryPublicFullAccess
      CustomActions:
        OnNodeConfigured:
          Sequence:
            - Script: 'https://raw.githubusercontent.com/aws-samples/aws-parallelcluster-post-install-scripts/main/docker/postinstall.sh'
            - Script: '${COMPUTE_NODE_BOOTSTRAP_SCRIPT}'

SharedStorage:
  - Name: shared-workspace-zfs
    StorageType: FsxOpenZfs
    MountDir: /fsx
    FsxOpenZfsSettings:
      VolumeId: ${FSX_OPENZFS_ROOT_VOLUME_ID}

  - Name: fsx-lustre
    MountDir: /lustre
    StorageType: FsxLustre
    FsxLustreSettings:
      FileSystemId: ${FSX_LUSTRE_ID}
      DeploymentType: PERSISTENT_1

Monitoring:
  DetailedMonitoring: true
  Logs:
    CloudWatch:
      Enabled: true
  Dashboards:
    CloudWatch:
      Enabled: true
EOF

echo "âœ… Cluster configuration file created: examples/configs/cluster-config.yaml"
```

### ì„¤ì • íŒŒì¼ ì£¼ìš” êµ¬ì„± ìš”ì†Œ

**HeadNode ì„¤ì •:**
- **InstanceType**: `m5.8xlarge` - 32 vCPU, 128GB RAM (Slurm ì»¨íŠ¸ë¡¤ëŸ¬ìš©)
- **SubnetId**: Public ì„œë¸Œë„· ì‚¬ìš© (Session Manager ì ‘ì†ìš©)
- **RootVolume**: 500GB (ì‚¬ìš©ì í™ˆ ë””ë ‰í† ë¦¬ í¬í•¨)
- **CustomActions**: Docker ë° Enroot/Pyxis ì„¤ì¹˜ ìŠ¤í¬ë¦½íŠ¸

**Compute Node ì„¤ì •:**
- **InstanceType**: `p5.8xlarge` - A10G GPU
- **SubnetId**: Private ì„œë¸Œë„· ì‚¬ìš©
- **MinCount/MaxCount**: 2 ë…¸ë“œ (í•„ìš”ì— ë”°ë¼ ì¡°ì • ê°€ëŠ¥)
- **EFA**: Enabled
- **LocalStorage**: 
  - `/scratch`: NVMe ê¸°ë°˜ ì„ì‹œ ìŠ¤í† ë¦¬ì§€
  - Root Volume: 512GB

**Slurm ì„¤ì •:**
- **ScaledownIdletime**: -1 (ìë™ ìŠ¤ì¼€ì¼ë‹¤ìš´ ë¹„í™œì„±í™”)
- **JobExclusiveAllocation**: true (ë…¸ë“œ ë…ì  í• ë‹¹)
- **QueueUpdateStrategy**: DRAIN

**SharedStorage:**
- **FSx OpenZFS** (`/fsx`): Home ë””ë ‰í† ë¦¬ ë° ì‚¬ìš©ì ë°ì´í„°
- **FSx Lustre** (`/lustre`): í•™ìŠµ ë°ì´í„°ì…‹

### ì„¤ì • íŒŒì¼ ê²€ì¦

ìƒì„±ëœ ì„¤ì • íŒŒì¼ì„ í™•ì¸í•©ë‹ˆë‹¤:

```bash
# ì„¤ì • íŒŒì¼ ë‚´ìš© í™•ì¸
cat examples/configs/cluster-config.yaml

# ParallelCluster CLIë¡œ ê²€ì¦
pcluster validate-config \
  --config-file examples/configs/cluster-config.yaml \
  --region ${AWS_REGION}
```

**ì˜ˆìƒ ì¶œë ¥:**
```json
{
  "message": "Configuration file is valid"
}
```

> âš ï¸ **ì£¼ì˜:**
> - í™˜ê²½ ë³€ìˆ˜ê°€ ì œëŒ€ë¡œ ì¹˜í™˜ë˜ì—ˆëŠ”ì§€ ê¼­ í™•ì¸í•˜ì„¸ìš”.

---

## 2.2 í´ëŸ¬ìŠ¤í„° ìƒì„±

### í´ëŸ¬ìŠ¤í„° ì´ë¦„ ì„¤ì •

```bash
export CLUSTER_NAME=ml-training-cluster
```

### í´ëŸ¬ìŠ¤í„° ìƒì„± ì‹œì‘

```bash
# í´ëŸ¬ìŠ¤í„° ìƒì„±
pcluster create-cluster \
  --cluster-name ${CLUSTER_NAME} \
  --cluster-configuration examples/configs/cluster-config.yaml \
  --region ${AWS_REGION}
```

**ì˜ˆìƒ ì¶œë ¥:**
```json
{
  "cluster": {
    "clusterName": "ml-training-cluster",
    "cloudformationStackStatus": "CREATE_IN_PROGRESS",
    "cloudformationStackArn": "arn:aws:cloudformation:us-east-1:123456789012:stack/...",
    "region": "us-east-1",
    "version": "3.14.0",
    "clusterStatus": "CREATE_IN_PROGRESS"
  }
}
```

### í´ëŸ¬ìŠ¤í„° ìƒì„± ìƒíƒœ ëª¨ë‹ˆí„°ë§

```bash
# í´ëŸ¬ìŠ¤í„° ìƒíƒœ í™•ì¸
pcluster describe-cluster \
  --cluster-name ${CLUSTER_NAME} \
  --region ${AWS_REGION} \
  --query 'clusterStatus' \
  --output text
```

**ì˜ˆìƒ ìƒíƒœ:**
- `CREATE_IN_PROGRESS`: ìƒì„± ì¤‘
- `CREATE_COMPLETE`: ìƒì„± ì™„ë£Œ âœ…
- `CREATE_FAILED`: ìƒì„± ì‹¤íŒ¨ âŒ

> â±ï¸ **ì˜ˆìƒ ì†Œìš” ì‹œê°„: ì•½ 25-35ë¶„**

### CloudFormation ìŠ¤íƒ í™•ì¸

```bash
# CloudFormation ìŠ¤íƒ ì´ë²¤íŠ¸ í™•ì¸
aws cloudformation describe-stack-events \
  --stack-name ${CLUSTER_NAME} \
  --region ${AWS_REGION} \
  --max-items 10 \
  --query 'StackEvents[*].[Timestamp,ResourceStatus,ResourceType,LogicalResourceId]' \
  --output table
```

---

## 2.3 í´ëŸ¬ìŠ¤í„° ì ‘ì† ë° ê²€ì¦

í´ëŸ¬ìŠ¤í„° ìƒì„±ì´ ì™„ë£Œë˜ë©´ Head Nodeì— ì ‘ì†í•˜ì—¬ í™˜ê²½ì„ í™•ì¸í•©ë‹ˆë‹¤.

### Head Node ì ‘ì†

Session Managerë¥¼ í†µí•´ Head Nodeì— ì ‘ì†í•©ë‹ˆë‹¤:

```bash
# SSH ì ‘ì†
pcluster ssh \
  --cluster-name ${CLUSTER_NAME} \
  --region ${AWS_REGION}
```

**ì˜ˆìƒ ì¶œë ¥:**
```
Starting session with SessionId: user-0a1b2c3d4e5f6g7h8
ubuntu@ip-10-0-0-123:~$
```

> ğŸ’¡ **Session Manager ì‚¬ìš©:**
> - SSH í‚¤ ì—†ì´ ì•ˆì „í•˜ê²Œ ì ‘ì†
> - IAM ê¸°ë°˜ ì¸ì¦
> - ì„¸ì…˜ ë¡œê·¸ ìë™ ê¸°ë¡

### ê¸°ë³¸ í™˜ê²½ í™•ì¸

Head Nodeì— ì ‘ì†í•œ í›„ ë‹¤ìŒ ëª…ë ¹ìœ¼ë¡œ í™˜ê²½ì„ í™•ì¸í•©ë‹ˆë‹¤:

```bash
# OS ì •ë³´ í™•ì¸
cat /etc/os-release

# ë§ˆìš´íŠ¸ëœ ê³µìœ  ìŠ¤í† ë¦¬ì§€ í™•ì¸
df -h | grep -E 'fsx|lustre'

# Docker ì„¤ì¹˜ í™•ì¸
docker --version

# Enroot ì„¤ì¹˜ í™•ì¸
enroot version

# Pyxis ì„¤ì¹˜ í™•ì¸ (Slurm í”ŒëŸ¬ê·¸ì¸)
ls -la /usr/local/lib/slurm/
```

**ì˜ˆìƒ ì¶œë ¥:**
```
NAME="Ubuntu"
VERSION="22.04.x LTS (Jammy Jellyfish)"

Filesystem                          Size  Used Avail Use% Mounted on
10.0.1.xxx@tcp:/fsvol-xxx          512G   64M  512G   1% /fsx
10.0.1.yyy@tcp:/yyyyyyy            1.1T  1.1M  1.1T   1% /lustre

Docker version 24.0.x
enroot version 3.4.1

spank_pyxis.so
```

---

## 2.4 Slurm ê¸°ë³¸ ì‚¬ìš©

### Slurm ë…¸ë“œ ìƒíƒœ í™•ì¸

```bash
# ë…¸ë“œ ì •ë³´ í™•ì¸
sinfo
```

**ì˜ˆìƒ ì¶œë ¥:**
```
PARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST
compute-gpu*  up   infinite      2  idle~ compute-gpu-distributed-ml-[1-4]
```

> ğŸ“ **ë…¸ë“œ ìƒíƒœ:**
> - `idle~`: ìœ íœ´ ìƒíƒœ, í•„ìš” ì‹œ ìë™ í”„ë¡œë¹„ì €ë‹
> - `alloc`: ì‘ì—…ì— í• ë‹¹ë¨
> - `mix`: ì¼ë¶€ ë¦¬ì†ŒìŠ¤ ì‚¬ìš© ì¤‘
> - `down`: ì‚¬ìš© ë¶ˆê°€

### Slurm íŒŒí‹°ì…˜ í™•ì¸

```bash
# íŒŒí‹°ì…˜ ì •ë³´
scontrol show partition compute-gpu
```

### ê°„ë‹¨í•œ nvidia-smi í…ŒìŠ¤íŠ¸

Compute Nodeë¥¼ í”„ë¡œë¹„ì €ë‹í•˜ê³  GPUë¥¼ í™•ì¸í•˜ëŠ” ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤:

```bash
# nvidia-smi í…ŒìŠ¤íŠ¸ ì‘ì—… ì œì¶œ
srun --partition=compute-gpu \
     --nodes=1 \
     --ntasks=1 \
     --gpus-per-node=1 \
     nvidia-smi
```

**ì˜ˆìƒ ì¶œë ¥:**
```
TBU
```

### ì‘ì—… í í™•ì¸

```bash
# ì‹¤í–‰ ì¤‘ì¸ ì‘ì—… í™•ì¸
squeue

# ë³¸ì¸ì˜ ì‘ì—…ë§Œ í™•ì¸
squeue -u $USER

# ì‘ì—… ìƒì„¸ ì •ë³´
scontrol show job <JOB_ID>
```

### Compute Node ìƒíƒœ í™•ì¸

```bash
# í”„ë¡œë¹„ì €ë‹ëœ ë…¸ë“œ í™•ì¸
sinfo

# íŠ¹ì • ë…¸ë“œ ìƒì„¸ ì •ë³´
scontrol show node compute-gpu-distributed-ml-1
```

---

## ë‹¤ìŒ ë‹¨ê³„

âœ… ParallelCluster ë°°í¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!

ì´ì œ **[3. ë¶„ì‚° í•™ìŠµ ì‹¤í–‰](./03-distributed-training.md)**ìœ¼ë¡œ ì§„í–‰í•˜ì—¬ ì‹¤ì œ í•™ìŠµ ì‘ì—…ì„ ì‹¤í–‰í•˜ì„¸ìš”.

---

### í™˜ê²½ ë³€ìˆ˜ ì €ì¥

```bash
# í´ëŸ¬ìŠ¤í„° ì´ë¦„ ì €ì¥
cat >> ~/pcluster-env.sh << EOF
export CLUSTER_NAME=${CLUSTER_NAME}
EOF
```

---

## ğŸ“š ë„¤ë¹„ê²Œì´ì…˜

| ì´ì „ | ìƒìœ„ | ë‹¤ìŒ |
|------|------|------|
| [â—€ ì‚¬ì „ ìš”êµ¬ì‚¬í•­](./01-prerequisites.md) | [ğŸ“‘ ëª©ì°¨](../README.md#-ê°€ì´ë“œ-ëª©ì°¨) | [ë¶„ì‚° í•™ìŠµ â–¶](./03-distributed-training.md) |