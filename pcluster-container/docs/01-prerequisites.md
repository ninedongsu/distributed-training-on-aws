# 1. ì‚¬ì „ ìš”êµ¬ì‚¬í•­

> ğŸ’¡ **ëª©í‘œ:** ParallelCluster ë°°í¬ì— í•„ìš”í•œ ë„êµ¬ ì„¤ì¹˜ ë° ì½”ì–´ ì¸í”„ë¼ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤.

> âš ï¸ **ì¤‘ìš”:** 
> - ì´ ê°€ì´ë“œì˜ ëŒ€ë¶€ë¶„ì˜ ì‘ì—…ì€ CLIë¥¼ í†µí•´ ìˆ˜í–‰ë˜ë©°, ì˜ˆì œëŠ” **us-east-1(N.Virginia)** ë¦¬ì „ì„ ê¸°ì¤€ìœ¼ë¡œ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤.
> - AWS CLI ì‚¬ìš©ì„ ìœ„í•œ **IAM ì‚¬ìš©ì ìê²© ì¦ëª…**(Access Key ID, Secret Access Key)ì´ ì¤€ë¹„ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.


## ëª©ì°¨

- [ê°œìš”](#ê°œìš”)
- [ì‚¬ì „ ì¤€ë¹„ì‚¬í•­](#ì‚¬ì „-ì¤€ë¹„ì‚¬í•­)
- [1. í•„ìš” ë„êµ¬ ì„¤ì¹˜](#1-í•„ìš”-ë„êµ¬-ì„¤ì¹˜)
  - [1.1 AWS CLI ì„¤ì¹˜](#11-aws-cli-ì„¤ì¹˜)
  - [1.2 ParallelCluster CLI ì„¤ì¹˜](#12-parallelcluster-cli-ì„¤ì¹˜)
  - [1.3 Session Manager Plugin ì„¤ì¹˜](#13-session-manager-plugin-ì„¤ì¹˜)
- [2. ë ˆí¬ì§€í† ë¦¬ í´ë¡ ](#2-ë ˆí¬ì§€í† ë¦¬-í´ë¡ )
- [3. ì½”ì–´ ì¸í”„ë¼ êµ¬ì„±](#3-ì½”ì–´-ì¸í”„ë¼-êµ¬ì„±)
  - [3.1 CloudFormation í…œí”Œë¦¿ ë°°í¬](#31-cloudformation-í…œí”Œë¦¿-ë°°í¬)
  - [3.2 ë°°í¬ ê²€ì¦](#32-ë°°í¬-ê²€ì¦)
- [4. ECR ë° ì˜ˆì œ Custom DLC ì¤€ë¹„](#4-ecr-ë°-ì˜ˆì œ-custom-dlc-ì¤€ë¹„)
  - [4.1 ECR Private Repository ìƒì„±](#41-ecr-private-repository-ìƒì„±)
  - [4.2 Custom DLC ì´ë¯¸ì§€ ë¹Œë“œ](#42-custom-dlc-ì´ë¯¸ì§€-ë¹Œë“œ)
  - [4.3 ECR ë¡œê·¸ì¸](#43-ecr-ë¡œê·¸ì¸)
  - [4.4 ì´ë¯¸ì§€ ë¹Œë“œ ë° í‘¸ì‹œ](#44-ì´ë¯¸ì§€-ë¹Œë“œ-ë°-í‘¸ì‹œ)
  - [4.5 ECR ì´ë¯¸ì§€ í™•ì¸](#45-ecr-ì´ë¯¸ì§€-í™•ì¸)
- [5. S3 ë²„í‚· ì¤€ë¹„](#5-s3-ë²„í‚·-ì¤€ë¹„)
  - [5.1 S3 ë²„í‚· ìƒì„±](#51-s3-ë²„í‚·-ìƒì„±)
  - [5.2 ë¶€íŠ¸ìŠ¤íŠ¸ë© ìŠ¤í¬ë¦½íŠ¸ ì—…ë¡œë“œ](#52-ë¶€íŠ¸ìŠ¤íŠ¸ë©-ìŠ¤í¬ë¦½íŠ¸-ì—…ë¡œë“œ)
  - [5.3 í•™ìŠµ ë°ì´í„°ì…‹ ì¤€ë¹„](#53-í•™ìŠµ-ë°ì´í„°ì…‹-ì¤€ë¹„)
  - [5.4 FSx Lustreì™€ S3 ì—°ë™ ì„¤ì •](#54-fsx-lustreì™€-s3-ì—°ë™-ì„¤ì •)
  - [5.5 í™˜ê²½ ë³€ìˆ˜ ì €ì¥](#55-í™˜ê²½-ë³€ìˆ˜-ì €ì¥)
  - [5.6 S3 ë²„í‚· êµ¬ì¡° ìµœì¢… í™•ì¸](#56-s3-ë²„í‚·-êµ¬ì¡°-ìµœì¢…-í™•ì¸)  
- [ë‹¤ìŒ ë‹¨ê³„](#ë‹¤ìŒ-ë‹¨ê³„)

---

## ê°œìš”

ì´ ë¬¸ì„œì—ì„œëŠ” ë‹¤ìŒ ì‘ì—…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤:

- âœ… AWS CLI ë° ParallelCluster CLI ì„¤ì¹˜
- âœ… VPC ë„¤íŠ¸ì›Œí¬, ê³µìœ  ìŠ¤í† ë¦¬ì§€ ë“± ì½”ì–´ ì¸í”„ë¼ êµ¬ì„±
- âœ… ECR ë¦¬í¬ì§€í† ë¦¬ ë° ì˜ˆì œ ì»¨í…Œì´ë„ˆ ì¤€ë¹„
- âœ… S3 ë²„í‚· ì¤€ë¹„

---

## 1. í•„ìš” ë„êµ¬ ì„¤ì¹˜

### 1.1 AWS CLI ì„¤ì¹˜

AWS CLIëŠ” AWS ë¦¬ì†ŒìŠ¤ë¥¼ ê´€ë¦¬í•˜ê¸° ìœ„í•œ ëª…ë ¹ì¤„ ë„êµ¬ì…ë‹ˆë‹¤.

#### Linux / macOS

```bash
# AWS CLI v2 ë‹¤ìš´ë¡œë“œ ë° ì„¤ì¹˜
curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
unzip awscliv2.zip
sudo ./aws/install
```

#### macOS (Homebrew ì‚¬ìš©)

```bash
brew install awscli
```

#### ì„¤ì¹˜ í™•ì¸

```bash
aws --version
```

**ì˜ˆìƒ ì¶œë ¥:**
```
aws-cli/2.x.x Python/3.x.x Linux/x86_64
```

#### AWS CLI êµ¬ì„±

```bash
aws configure
```

ë‹¤ìŒ ì •ë³´ë¥¼ ì…ë ¥í•©ë‹ˆë‹¤:

```
AWS Access Key ID [None]: YOUR_ACCESS_KEY
AWS Secret Access Key [None]: YOUR_SECRET_KEY
Default region name [None]: us-east-1
Default output format [None]: json
```

#### êµ¬ì„± í™•ì¸

```bash
# í˜„ì¬ ê³„ì • ì •ë³´ í™•ì¸
aws sts get-caller-identity
```

**ì˜ˆìƒ ì¶œë ¥:**
```json
{
    "UserId": "AIDAXXXXXXXXXXXXXXXXX",
    "Account":[REDACTED:BANK_ACCOUNT_NUMBER]12",
    "Arn": "arn:aws:iam::123456789012:user/your-username"
}
```

---

### 1.2 ParallelCluster CLI ì„¤ì¹˜

ParallelCluster CLIëŠ” HPC í´ëŸ¬ìŠ¤í„°ë¥¼ ìƒì„±í•˜ê³  ê´€ë¦¬í•˜ëŠ” ë„êµ¬ì…ë‹ˆë‹¤.

#### Python ë° pip í™•ì¸

```bash
# Python 3.8 ì´ìƒ í•„ìš”
python3 --version

# pip í™•ì¸
pip3 --version
```

#### ParallelCluster CLI ì„¤ì¹˜

```bash
pip3 install --upgrade "aws-parallelcluster==3.14.0"
```

> ğŸ“ **ì°¸ê³ :** 
> - íŠ¹ì • ë²„ì „(3.14.0)ì„ ì„¤ì¹˜í•˜ì—¬ ê°€ì´ë“œì™€ì˜ í˜¸í™˜ì„± ë³´ì¥
> - ìµœì‹  ë²„ì „ì€ [ê³µì‹ ë¦´ë¦¬ìŠ¤](https://github.com/aws/aws-parallelcluster/releases)ì—ì„œ í™•ì¸

#### ì„¤ì¹˜ í™•ì¸

```bash
pcluster version
```

**ì˜ˆìƒ ì¶œë ¥:**
```json
{
  "version": "3.14.0"
}
```

---

### 1.3 Session Manager Plugin ì„¤ì¹˜

Session ManagerëŠ” SSH í‚¤ ì—†ì´ ì•ˆì „í•˜ê²Œ EC2 ì¸ìŠ¤í„´ìŠ¤ì— ì ‘ì†í•  ìˆ˜ ìˆëŠ” AWS Systems Managerì˜ ê¸°ëŠ¥ì…ë‹ˆë‹¤. ì´í›„ ì‹¤ìŠµì—ì„œ ParallelClusterì˜ Head Nodeì— ì ‘ì†í•  ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤.

> ğŸ’¡ **ì™œ Session Managerë¥¼ ì‚¬ìš©í•˜ë‚˜ìš”?**
> - SSH í‚¤ ê´€ë¦¬ ë¶ˆí•„ìš”
> - ë³´ì•ˆ ê·¸ë£¹ì—ì„œ 22ë²ˆ í¬íŠ¸ ê°œë°© ë¶ˆí•„ìš”
> - AWS IAM ê¸°ë°˜ ì ‘ê·¼ ì œì–´
> - ì„¸ì…˜ ë¡œê·¸ ìë™ ê¸°ë¡

#### AL2023 and RHEL 8,9

```bash
# Session Manager Plugin ì„¤ì¹˜
sudo dnf install -y https://s3.amazonaws.com/session-manager-downloads/plugin/latest/linux_64bit/session-manager-plugin.rpm
```

#### Ubuntu / Debian

```bash
# ë‹¤ìš´ë¡œë“œ
curl "https://s3.amazonaws.com/session-manager-downloads/plugin/latest/ubuntu_64bit/session-manager-plugin.deb" -o "session-manager-plugin.deb"

# ì„¤ì¹˜
sudo dpkg -i session-manager-plugin.deb
```

#### macOS

```bash
# ë‹¤ìš´ë¡œë“œ ë° ì„¤ì¹˜
curl "https://s3.amazonaws.com/session-manager-downloads/plugin/latest/mac/sessionmanager-bundle.zip" -o "sessionmanager-bundle.zip"
unzip sessionmanager-bundle.zip
sudo ./sessionmanager-bundle/install -i /usr/local/sessionmanagerplugin -b /usr/local/bin/session-manager-plugin
```

#### ì„¤ì¹˜ í™•ì¸

```bash
session-manager-plugin --version
```

**ì˜ˆìƒ ì¶œë ¥:**
```
1.2.xxx
```

> ğŸ“ **ì°¸ê³ :** 
> - ìì„¸í•œ ì„¤ì¹˜ ë°©ë²•ì€ [ê³µì‹ ë¬¸ì„œ](https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager-working-with-install-plugin.html)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.

---

## 2. ë ˆí¬ì§€í† ë¦¬ í´ë¡ 

ì´ ê°€ì´ë“œì˜ ì˜ˆì œ íŒŒì¼ë“¤(ì„¤ì • íŒŒì¼, Dockerfile, ìŠ¤í¬ë¦½íŠ¸ ë“±)ì„ ì‚¬ìš©í•˜ê¸° ìœ„í•´ ë ˆí¬ì§€í† ë¦¬ë¥¼ í´ë¡ í•©ë‹ˆë‹¤.

```bash
# ì‘ì—… ë””ë ‰í† ë¦¬ë¡œ ì´ë™
cd ~

# ë ˆí¬ì§€í† ë¦¬ í´ë¡ 
git clone https://github.com/ninedongsu/distributed-training-on-aws.git

# í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ë¡œ ì´ë™
cd distributed-training-on-aws/pcluster-container
```

**ë””ë ‰í† ë¦¬ êµ¬ì¡° í™•ì¸:**

```bash
ls -la
```

**ì˜ˆìƒ ì¶œë ¥:**
```
drwxr-xr-x  docs/
drwxr-xr-x  examples/
  â”œâ”€â”€ configs/
  â”œâ”€â”€ containers/
  â”œâ”€â”€ scripts/
  â””â”€â”€ templates/
drwxr-xr-x  images/
-rw-r--r--  README.md
```

> ğŸ“ **ì°¸ê³ :** 
> - ì´í›„ ëª¨ë“  ëª…ë ¹ì€ `~/distributed-training-on-aws/pcluster-container` ë””ë ‰í† ë¦¬ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í•©ë‹ˆë‹¤.
> - ë ˆí¬ì§€í† ë¦¬ URLì€ ì‹¤ì œ GitHub ì£¼ì†Œë¡œ ë³€ê²½í•˜ì„¸ìš”.

---

## 3. ì½”ì–´ ì¸í”„ë¼ êµ¬ì„±

ParallelClusterë¥¼ ë°°í¬í•˜ê¸° ì „ì— í•„ìš”í•œ ë„¤íŠ¸ì›Œí¬ ë° ìŠ¤í† ë¦¬ì§€ ì¸í”„ë¼ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤.

### 3.1 CloudFormation í…œí”Œë¦¿ ë°°í¬

AWSì—ì„œ ì œê³µí•˜ëŠ” ì‚¬ì „ êµ¬ì„±ëœ í…œí”Œë¦¿ì„ ì‚¬ìš©í•˜ì—¬ ë‹¤ìŒ ë¦¬ì†ŒìŠ¤ë¥¼ ìë™ìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤:

- **VPC ë° ì„œë¸Œë„·**: Public/Private ì„œë¸Œë„· í¬í•¨
- **ë³´ì•ˆ ê·¸ë£¹**: í´ëŸ¬ìŠ¤í„° ê°„ í†µì‹ ì„ ìœ„í•œ ê·œì¹™ (EFA ì§€ì›)
- **FSx for Lustre**: ê³ ì„±ëŠ¥ ê³µìœ  íŒŒì¼ ì‹œìŠ¤í…œ (í•™ìŠµ ë°ì´í„°ìš©)
- **FSx for OpenZFS**: Home ë””ë ‰í† ë¦¬ìš© íŒŒì¼ ì‹œìŠ¤í…œ
- **NAT Gateway**: Private ì„œë¸Œë„·ì˜ ì¸í„°ë„· ì—°ê²°
- **S3 Endpoint**: S3 ì ‘ê·¼ì„ ìœ„í•œ VPC ì—”ë“œí¬ì¸íŠ¸

> ğŸ“ **í…œí”Œë¦¿ ì •ë³´:**
> - í…œí”Œë¦¿ ì „ì²´ ë‚´ìš©ì€ [examples/templates/parallelcluster-prerequisites.yaml](../examples/templates/parallelcluster-prerequisites.yaml)ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
> - FSx LustreëŠ” **PERSISTENT_1** ë°°í¬ ìœ í˜•ì„ ì‚¬ìš©í•˜ë©°, 100 ë˜ëŠ” 200 MB/s/TiBì˜ ì²˜ë¦¬ëŸ‰ì„ ì§€ì›í•©ë‹ˆë‹¤.

#### í™˜ê²½ ë³€ìˆ˜ ì„¤ì •

```bash
export AWS_REGION=us-east-1  # ì›í•˜ëŠ” ë¦¬ì „
export STACK_NAME=parallelcluster-prerequisites
```

#### ì‚¬ìš© ê°€ëŠ¥í•œ Availability Zone í™•ì¸

í…œí”Œë¦¿ì—ì„œ AZ ì§€ì •ì´ í•„ìˆ˜ì´ë¯€ë¡œ, ë¨¼ì € ì‚¬ìš© ê°€ëŠ¥í•œ AZë¥¼ í™•ì¸í•©ë‹ˆë‹¤:

```bash
# us-east-1 ë¦¬ì „ì˜ AZ ëª©ë¡ ì¡°íšŒ
aws ec2 describe-availability-zones \
  --region ${AWS_REGION} \
  --query 'AvailabilityZones[*].[ZoneName,State]' \
  --output table
```

**ì˜ˆìƒ ì¶œë ¥:**
```
-----------------------------
|DescribeAvailabilityZones|
+--------------+------------+
|  us-east-1a  |  available |
|  us-east-1b  |  available |
|  us-east-1c  |  available |
|  us-east-1d  |  available |
|  us-east-1e  |  available |
|  us-east-1f  |  available |
+--------------+------------+
```

ì›í•˜ëŠ” AZë¥¼ í™˜ê²½ ë³€ìˆ˜ë¡œ ì„¤ì •:

```bash
export PRIMARY_AZ=us-east-1f  # ì›í•˜ëŠ” AZ ì„ íƒ
```

#### CloudFormation ìŠ¤íƒ ìƒì„±

**ì˜µì…˜ 1: ë¡œì»¬ í…œí”Œë¦¿ íŒŒì¼ ì‚¬ìš© (ê¶Œì¥)**

```bash
# í…œí”Œë¦¿ íŒŒì¼ ê²½ë¡œ ì„¤ì • (ì´ ë ˆí¬ì§€í† ë¦¬ ê¸°ì¤€)
export TEMPLATE_FILE=../examples/templates/parallelcluster-prerequisites.yaml

# ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ìƒì„±
aws cloudformation create-stack \
  --stack-name ${STACK_NAME} \
  --template-body file://${TEMPLATE_FILE} \
  --region ${AWS_REGION} \
  --parameters \
    ParameterKey=PrimarySubnetAZ,ParameterValue=${PRIMARY_AZ}
```

**ì˜µì…˜ 2: S3ì— ì—…ë¡œë“œí•˜ì—¬ ì‚¬ìš©**

ìì‹ ì˜ S3 ë²„í‚·ì— í…œí”Œë¦¿ì„ ì—…ë¡œë“œí•˜ê³  ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```bash
# S3 ë²„í‚· ìƒì„± (ì´ë¯¸ ìˆë‹¤ë©´ skip)
export TEMPLATE_BUCKET=my-cloudformation-templates-${AWS_REGION}
aws s3 mb s3://${TEMPLATE_BUCKET} --region ${AWS_REGION}

# í…œí”Œë¦¿ ì—…ë¡œë“œ
aws s3 cp ../examples/templates/parallelcluster-prerequisites.yaml \
  s3://${TEMPLATE_BUCKET}/parallelcluster-prerequisites.yaml

# ìŠ¤íƒ ìƒì„±
aws cloudformation create-stack \
  --stack-name ${STACK_NAME} \
  --template-url https://${TEMPLATE_BUCKET}.s3.amazonaws.com/parallelcluster-prerequisites.yaml \
  --region ${AWS_REGION} \
  --parameters \
    ParameterKey=PrimarySubnetAZ,ParameterValue=${PRIMARY_AZ}
```

**ì»¤ìŠ¤í…€ ì„¤ì •ìœ¼ë¡œ ìƒì„±:**

```bash
aws cloudformation create-stack \
  --stack-name ${STACK_NAME} \
  --template-body file://${TEMPLATE_FILE} \
  --region ${AWS_REGION} \
  --parameters \
    ParameterKey=VPCName,ParameterValue="ML Training VPC" \
    ParameterKey=PrimarySubnetAZ,ParameterValue=${PRIMARY_AZ} \
    ParameterKey=Capacity,ParameterValue=2400 \
    ParameterKey=PerUnitStorageThroughput,ParameterValue=200 \
    ParameterKey=Compression,ParameterValue=LZ4 \
    ParameterKey=LustreVersion,ParameterValue=2.15 \
    ParameterKey=OpenZFSCapacity,ParameterValue=512 \
    ParameterKey=OpenZFSThroughput,ParameterValue=320 \
    ParameterKey=CreateS3Endpoint,ParameterValue=true
```

#### ë°°í¬ ì§„í–‰ ìƒí™© ëª¨ë‹ˆí„°ë§

```bash
# ìŠ¤íƒ ìƒì„± ìƒíƒœ í™•ì¸
aws cloudformation describe-stacks \
  --stack-name ${STACK_NAME} \
  --region ${AWS_REGION} \
  --query 'Stacks[0].StackStatus' \
  --output text
```

**ì˜ˆìƒ ìƒíƒœ:**
- `CREATE_IN_PROGRESS`: ìƒì„± ì¤‘
- `CREATE_COMPLETE`: ìƒì„± ì™„ë£Œ âœ…
- `CREATE_FAILED`: ìƒì„± ì‹¤íŒ¨ âŒ

> â±ï¸ **ì˜ˆìƒ ì†Œìš” ì‹œê°„:** ì•½ 15-20ë¶„

#### ì‹¤ì‹œê°„ ì´ë²¤íŠ¸ ëª¨ë‹ˆí„°ë§

```bash
# ì´ë²¤íŠ¸ ë¡œê·¸ í™•ì¸ (ê³„ì† ì—…ë°ì´íŠ¸)
watch -n 10 "aws cloudformation describe-stack-events \
  --stack-name ${STACK_NAME} \
  --region ${AWS_REGION} \
  --max-items 5 \
  --query 'StackEvents[*].[Timestamp,ResourceStatus,ResourceType,LogicalResourceId]' \
  --output table"
```

ë˜ëŠ” ìŠ¤íƒ ì™„ë£Œê¹Œì§€ ëŒ€ê¸°:

```bash
# ìŠ¤íƒ ìƒì„± ì™„ë£Œê¹Œì§€ ëŒ€ê¸°
aws cloudformation wait stack-create-complete \
  --stack-name ${STACK_NAME} \
  --region ${AWS_REGION}
```

---

### 3.2 ë°°í¬ ê²€ì¦

ìŠ¤íƒ ìƒì„±ì´ ì™„ë£Œë˜ë©´ ìƒì„±ëœ ë¦¬ì†ŒìŠ¤ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.

#### ì¶œë ¥ê°’ í™•ì¸

```bash
# CloudFormation ì¶œë ¥ê°’ ì¡°íšŒ
aws cloudformation describe-stacks \
  --stack-name ${STACK_NAME} \
  --region ${AWS_REGION} \
  --query 'Stacks[0].Outputs' \
  --output table
```

**ì£¼ìš” ì¶œë ¥ê°’:**

| OutputKey | ì„¤ëª… | ì˜ˆì‹œ |
|-----------|------|------|
| `VPC` | VPC ID | vpc-0a1b2c3d4e5f6g7h8 |
| `PublicSubnet` | Public ì„œë¸Œë„· ID | subnet-0a1b2c3d |
| `PrimaryPrivateSubnet` | Private ì„œë¸Œë„· ID | subnet-4e5f6g7h |
| `SecurityGroup` | ë³´ì•ˆ ê·¸ë£¹ ID | sg-0a1b2c3d4e5f6g7h8 |
| `FSxLustreFilesystemId` | FSx Lustre íŒŒì¼ ì‹œìŠ¤í…œ ID | fs-0a1b2c3d4e5f6g7h8 |
| `FSxLustreFilesystemMountname` | FSx Lustre ë§ˆìš´íŠ¸ ì´ë¦„ | xxxxxxxx |
| `FSxLustreFilesystemDNSname` | FSx Lustre DNS ì´ë¦„ | fs-xxx.fsx.us-east-1.amazonaws.com |
| `FSxORootVolumeId` | FSx OpenZFS ë£¨íŠ¸ ë³¼ë¥¨ ID | fsvol-0a1b2c3d4e5f6g7h8 |

#### ì¶œë ¥ê°’ì„ í™˜ê²½ ë³€ìˆ˜ë¡œ ì €ì¥

ë‚˜ì¤‘ì— ì‚¬ìš©í•˜ê¸° ìœ„í•´ ì¶œë ¥ê°’ì„ í™˜ê²½ ë³€ìˆ˜ë¡œ ì €ì¥í•©ë‹ˆë‹¤:

```bash
# ì¶œë ¥ê°’ ì¶”ì¶œ ë° í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
export VPC_ID=$(aws cloudformation describe-stacks \
  --stack-name ${STACK_NAME} \
  --region ${AWS_REGION} \
  --query 'Stacks[0].Outputs[?OutputKey==`VPC`].OutputValue' \
  --output text)

export PUBLIC_SUBNET_ID=$(aws cloudformation describe-stacks \
  --stack-name ${STACK_NAME} \
  --region ${AWS_REGION} \
  --query 'Stacks[0].Outputs[?OutputKey==`PublicSubnet`].OutputValue' \
  --output text)

export PRIVATE_SUBNET_ID=$(aws cloudformation describe-stacks \
  --stack-name ${STACK_NAME} \
  --region ${AWS_REGION} \
  --query 'Stacks[0].Outputs[?OutputKey==`PrimaryPrivateSubnet`].OutputValue' \
  --output text)

export SECURITY_GROUP_ID=$(aws cloudformation describe-stacks \
  --stack-name ${STACK_NAME} \
  --region ${AWS_REGION} \
  --query 'Stacks[0].Outputs[?OutputKey==`SecurityGroup`].OutputValue' \
  --output text)

export FSX_LUSTRE_ID=$(aws cloudformation describe-stacks \
  --stack-name ${STACK_NAME} \
  --region ${AWS_REGION} \
  --query 'Stacks[0].Outputs[?OutputKey==`FSxLustreFilesystemId`].OutputValue' \
  --output text)

export FSX_LUSTRE_MOUNT_NAME=$(aws cloudformation describe-stacks \
  --stack-name ${STACK_NAME} \
  --region ${AWS_REGION} \
  --query 'Stacks[0].Outputs[?OutputKey==`FSxLustreFilesystemMountname`].OutputValue' \
  --output text)

export FSX_LUSTRE_DNS=$(aws cloudformation describe-stacks \
  --stack-name ${STACK_NAME} \
  --region ${AWS_REGION} \
  --query 'Stacks[0].Outputs[?OutputKey==`FSxLustreFilesystemDNSname`].OutputValue' \
  --output text)

export FSX_OPENZFS_ROOT_VOLUME_ID=$(aws cloudformation describe-stacks \
  --stack-name ${STACK_NAME} \
  --region ${AWS_REGION} \
  --query 'Stacks[0].Outputs[?OutputKey==`FSxORootVolumeId`].OutputValue' \
  --output text)

# í™•ì¸
echo "VPC ID: $VPC_ID"
echo "Public Subnet: $PUBLIC_SUBNET_ID"
echo "Private Subnet: $PRIVATE_SUBNET_ID"
echo "Security Group: $SECURITY_GROUP_ID"
echo "FSx Lustre ID: $FSX_LUSTRE_ID"
echo "FSx Lustre Mount Name: $FSX_LUSTRE_MOUNT_NAME"
echo "FSx Lustre DNS: $FSX_LUSTRE_DNS"
echo "FSx OpenZFS Root Volume ID: $FSX_OPENZFS_ROOT_VOLUME_ID"
```

> ğŸ’¡ **íŒ:** ì´ í™˜ê²½ ë³€ìˆ˜ë“¤ì„ íŒŒì¼ë¡œ ì €ì¥í•˜ì—¬ ì„¸ì…˜ ê°„ì— ìœ ì§€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```bash
# í™˜ê²½ ë³€ìˆ˜ë¥¼ íŒŒì¼ë¡œ ì €ì¥
cat > ~/pcluster-env.sh << EOF
export AWS_REGION=${AWS_REGION}
export STACK_NAME=${STACK_NAME}
export PRIMARY_AZ=${PRIMARY_AZ}
export VPC_ID=${VPC_ID}
export PUBLIC_SUBNET_ID=${PUBLIC_SUBNET_ID}
export PRIVATE_SUBNET_ID=${PRIVATE_SUBNET_ID}
export SECURITY_GROUP_ID=${SECURITY_GROUP_ID}
export FSX_LUSTRE_ID=${FSX_LUSTRE_ID}
export FSX_LUSTRE_MOUNT_NAME=${FSX_LUSTRE_MOUNT_NAME}
export FSX_LUSTRE_DNS=${FSX_LUSTRE_DNS}
export FSX_OPENZFS_ROOT_VOLUME_ID=${FSX_OPENZFS_ROOT_VOLUME_ID}
EOF

# ë‚˜ì¤‘ì— ì‚¬ìš© ì‹œ
source ~/pcluster-env.sh
```

#### ë¦¬ì†ŒìŠ¤ ìƒíƒœ í™•ì¸ (ì„ íƒ ì‚¬í•­)

**VPC í™•ì¸:**
```bash
aws ec2 describe-vpcs \
  --vpc-ids ${VPC_ID} \
  --region ${AWS_REGION} \
  --query 'Vpcs[0].[VpcId,CidrBlock,State]' \
  --output table
```

**ì„œë¸Œë„· í™•ì¸:**
```bash
aws ec2 describe-subnets \
  --subnet-ids ${PUBLIC_SUBNET_ID} ${PRIVATE_SUBNET_ID} \
  --region ${AWS_REGION} \
  --query 'Subnets[*].[SubnetId,CidrBlock,AvailabilityZone,MapPublicIpOnLaunch]' \
  --output table
```

**ë³´ì•ˆ ê·¸ë£¹ í™•ì¸:**
```bash
aws ec2 describe-security-groups \
  --group-ids ${SECURITY_GROUP_ID} \
  --region ${AWS_REGION} \
  --query 'SecurityGroups[0].[GroupId,GroupName,Description]' \
  --output table
```

**FSx for Lustre í™•ì¸:**
```bash
aws fsx describe-file-systems \
  --file-system-ids ${FSX_LUSTRE_ID} \
  --region ${AWS_REGION} \
  --query 'FileSystems[0].[FileSystemId,Lifecycle,StorageCapacity,FileSystemTypeVersion]' \
  --output table
```

**ì˜ˆìƒ ì¶œë ¥:**
```
--------------------------------------------------------------------
|                     DescribeFileSystems                          |
+----------------------+------------+--------+---------------------+
|  fs-0a1b2c3d4e5f... |  AVAILABLE | 1200   | 2.15                |
+----------------------+------------+--------+---------------------+
```

**FSx for OpenZFS í™•ì¸:**
```bash
aws fsx describe-volumes \
  --volume-ids ${FSX_OPENZFS_ROOT_VOLUME_ID} \
  --region ${AWS_REGION} \
  --query 'Volumes[0].[VolumeId,Lifecycle,VolumeType]' \
  --output table
```

---

## 4. ECR ë° ì˜ˆì œ Custom DLC ì¤€ë¹„

ì»¨í…Œì´ë„ˆ ê¸°ë°˜ í•™ìŠµì„ ìœ„í•´ Amazon ECR(Elastic Container Registry)ì— ì»¤ìŠ¤í…€ Deep Learning Container(DLC)ë¥¼ ì¤€ë¹„í•©ë‹ˆë‹¤.

> ğŸ’¡ **Custom DLCë€?**
> - AWSì—ì„œ ì œê³µí•˜ëŠ” ê³µì‹ Deep Learning Containerë¥¼ ë² ì´ìŠ¤ë¡œ ì‚¬ìš©
> - ì¶”ê°€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë° í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ë¥¼ í¬í•¨í•˜ì—¬ ì»¤ìŠ¤í„°ë§ˆì´ì§•
> - ECRì— ì €ì¥í•˜ì—¬ í´ëŸ¬ìŠ¤í„°ì—ì„œ ì‚¬ìš©

### 4.1 ECR Private Repository ìƒì„±

í•™ìŠµìš© ì»¨í…Œì´ë„ˆ ì´ë¯¸ì§€ë¥¼ ì €ì¥í•  í”„ë¼ì´ë¹— ë¦¬í¬ì§€í† ë¦¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

#### ë¦¬í¬ì§€í† ë¦¬ ì´ë¦„ ì„¤ì •

```bash
export ECR_REPO_NAME=pytorch-training-custom
```

#### ECR ë¦¬í¬ì§€í† ë¦¬ ìƒì„±

```bash
aws ecr create-repository \
  --repository-name ${ECR_REPO_NAME} \
  --region ${AWS_REGION}
```

#### ë¦¬í¬ì§€í† ë¦¬ URI ì €ì¥

```bash
export ECR_REPO_URI=$(aws ecr describe-repositories \
  --repository-names ${ECR_REPO_NAME} \
  --region ${AWS_REGION} \
  --query 'repositories[0].repositoryUri' \
  --output text)

echo "ECR Repository URI: ${ECR_REPO_URI}"
```

#### ë¦¬í¬ì§€í† ë¦¬ í™•ì¸

```bash
aws ecr describe-repositories \
  --repository-names ${ECR_REPO_NAME} \
  --region ${AWS_REGION} \
  --query 'repositories[0].[repositoryName,repositoryUri,createdAt]' \
  --output table
```

---

### 4.2 Custom DLC ì´ë¯¸ì§€ ë¹Œë“œ

AWSì—ì„œ ì œê³µí•˜ëŠ” PyTorch DLCë¥¼ ë² ì´ìŠ¤ë¡œ ë¶„ì‚° í•™ìŠµì— í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì¶”ê°€í•œ ì»¤ìŠ¤í…€ ì´ë¯¸ì§€ë¥¼ ë¹Œë“œí•©ë‹ˆë‹¤.

#### Dockerfile ë° í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ ìœ„ì¹˜

> ğŸ“ **íŒŒì¼ ìœ„ì¹˜:** 
> - `examples/containers/pytorch/Dockerfile`
> - `examples/containers/pytorch/ds_config.json`
> - `examples/containers/pytorch/train_distributed_deepspeed.py`

#### Dockerfile ë‚´ìš©

```dockerfile
FROM public.ecr.aws/deep-learning-containers/pytorch-training:2.5.1-gpu-py311-cu124-ubuntu22.04-ec2-v1.30

RUN apt-get update && apt-get install -y \
    openssh-server \
    pdsh \
    net-tools \
    && rm -rf /var/lib/apt/lists/*

RUN pip install \
    transformers>=4.37.0 \
    flash-attn --no-build-isolation \
    deepspeed \
    accelerate \
    datasets

WORKDIR /workspace

COPY ds_config.json /workspace/
COPY train_distributed_deepspeed.py /workspace/
```

**Dockerfile êµ¬ì„± ì„¤ëª…:**

| í•­ëª© | ì„¤ëª… |
|------|------|
| **Base Image** | AWS ê³µì‹ PyTorch 2.5.1 DLC (CUDA 12.4, Python 3.11, Ubuntu 22.04) |
| **Python Libraries** | `transformers`: Hugging Face Transformers<br>`flash-attn`: Flash Attention ìµœì í™”<br>`deepspeed`: DeepSpeed ë¶„ì‚° í•™ìŠµ<br>`accelerate`: Hugging Face Accelerate<br>`datasets`: ë°ì´í„°ì…‹ ë¡œë“œ |
| **Working Directory** | `/workspace`: í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ë””ë ‰í† ë¦¬ |
| **Training Scripts** | `ds_config.json`: DeepSpeed ì„¤ì • íŒŒì¼<br>`train_distributed_deepspeed.py`: ë¶„ì‚° í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ |

> ğŸ’¡ **ì™œ AWS DLCë¥¼ ì‚¬ìš©í•˜ë‚˜ìš”?**
> - AWSì— ìµœì í™”ëœ PyTorch ë° CUDA ì„¤ì •
> - EFA(Elastic Fabric Adapter) ì§€ì›
> - NCCL ìµœì í™”
> - ì •ê¸°ì ì¸ ë³´ì•ˆ ì—…ë°ì´íŠ¸ ë° íŒ¨ì¹˜

#### ì‘ì—… ë””ë ‰í† ë¦¬ ì´ë™

```bash
# Dockerfileì´ ìˆëŠ” ë””ë ‰í† ë¦¬ë¡œ ì´ë™
cd examples/containers/pytorch/
```

> ğŸ“ **ì°¸ê³ :** 
> - `ds_config.json`ê³¼ `train_distributed_deepspeed.py` íŒŒì¼ì´ ê°™ì€ ë””ë ‰í† ë¦¬ì— ìˆì–´ì•¼ í•©ë‹ˆë‹¤.
> - ì‹¤ì œ íŒŒì¼ ë‚´ìš©ì€ [examples/containers/pytorch/](../examples/containers/pytorch/) ë””ë ‰í† ë¦¬ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.

---

### 4.3 ECR ë¡œê·¸ì¸

ì»¨í…Œì´ë„ˆ ì´ë¯¸ì§€ë¥¼ í‘¸ì‹œí•˜ê¸° ì „ì— ECRì— ë¡œê·¸ì¸í•©ë‹ˆë‹¤.

```bash
# ECR ë¡œê·¸ì¸
aws ecr get-login-password --region ${AWS_REGION} | \
  docker login --username AWS --password-stdin ${ECR_REPO_URI}
```

**ì˜ˆìƒ ì¶œë ¥:**
```
Login Succeeded
```

---

### 4.4 ì´ë¯¸ì§€ ë¹Œë“œ ë° í‘¸ì‹œ

#### ì´ë¯¸ì§€ ë¹Œë“œ

```bash
# ì´ë¯¸ì§€ íƒœê·¸ ì„¤ì •
export IMAGE_TAG=latest

# Docker ì´ë¯¸ì§€ ë¹Œë“œ
docker build -t ${ECR_REPO_NAME}:${IMAGE_TAG} .
```

**ë¹Œë“œ ì§„í–‰ ìƒí™©:**
```
[+] Building 245.3s (10/10) FINISHED
 => [internal] load build definition from Dockerfile
 => => transferring dockerfile: 425B
 => [internal] load .dockerignore
 => [1/5] FROM public.ecr.aws/deep-learning-containers/pytorch-training:2.5.1...
 => [2/5] RUN apt-get update && apt-get install -y openssh-server...
 => [3/5] RUN pip install transformers>=4.37.0...
 => [4/5] WORKDIR /workspace
 => [5/5] COPY ds_config.json /workspace/
 => [6/5] COPY train_distributed_deepspeed.py /workspace/
 => exporting to image
 => => naming to docker.io/library/pytorch-training-custom:latest
```

> â±ï¸ **ì˜ˆìƒ ì†Œìš” ì‹œê°„:** ì•½ 5-10ë¶„

#### ë¡œì»¬ ì´ë¯¸ì§€ í™•ì¸

```bash
docker images ${ECR_REPO_NAME}
```

**ì˜ˆìƒ ì¶œë ¥:**
```
REPOSITORY                TAG       IMAGE ID       CREATED          SIZE
pytorch-training-custom   latest    9d2cf2ea9849   27 seconds ago   20.3GB
```

#### ECR íƒœê·¸ ì§€ì •

```bash
docker tag ${ECR_REPO_NAME}:${IMAGE_TAG} ${ECR_REPO_URI}:${IMAGE_TAG}
```

#### ECRì— ì´ë¯¸ì§€ í‘¸ì‹œ

```bash
docker push ${ECR_REPO_URI}:${IMAGE_TAG}
```

**í‘¸ì‹œ ì§„í–‰ ìƒí™©:**
```
The push refers to repository [123456789012.dkr.ecr.us-east-1.amazonaws.com/pytorch-training-custom]
5f70bf18a086: Pushed
a3b5c80a4eba: Pushed
7f18b442972b: Pushed
3ce63537e70c: Pushed
latest: digest: sha256:1234567890abcdef... size: 4321
```

> â±ï¸ **ì˜ˆìƒ ì†Œìš” ì‹œê°„:** ì•½ 10-20ë¶„

---

### 4.5 ECR ì´ë¯¸ì§€ í™•ì¸

#### ECRì— í‘¸ì‹œëœ ì´ë¯¸ì§€ í™•ì¸

```bash
aws ecr describe-images \
  --repository-name ${ECR_REPO_NAME} \
  --region ${AWS_REGION} \
  --query 'imageDetails[*].[imageTags[0],imagePushedAt,imageSizeInBytes]' \
  --output table
```

**ì˜ˆìƒ ì¶œë ¥:**
```
------------------------------------------------------------
|                    DescribeImages                        |
+----------+---------------------------+-------------------+
|  latest  |  2024-01-01T12:00:00+00:00|  15234567890     |
+----------+---------------------------+-------------------+
```

---

## 5. S3 ë²„í‚· ì¤€ë¹„

ParallelCluster ìš´ì˜ ë° í•™ìŠµ ë°ì´í„° ì €ì¥ì„ ìœ„í•œ S3 ë²„í‚·ì„ ì¤€ë¹„í•©ë‹ˆë‹¤.

> ğŸ’¡ **S3 ë²„í‚· ìš©ë„:**
> - **ë¶€íŠ¸ìŠ¤íŠ¸ë© ìŠ¤í¬ë¦½íŠ¸**: í´ëŸ¬ìŠ¤í„° ìƒì„± ì‹œ ìë™ ì‹¤í–‰ë  ìŠ¤í¬ë¦½íŠ¸
> - **í•™ìŠµ ë°ì´í„°ì…‹**: FSx Lustreì™€ ì—°ë™í•˜ì—¬ ì‚¬ìš©
> - **í•™ìŠµ ê²°ê³¼**: ì²´í¬í¬ì¸íŠ¸, ë¡œê·¸, ëª¨ë¸ ì €ì¥

### 5.1 S3 ë²„í‚· ìƒì„±

#### ë²„í‚· ì´ë¦„ ì„¤ì •

S3 ë²„í‚· ì´ë¦„ì€ ì „ì—­ì ìœ¼ë¡œ ê³ ìœ í•´ì•¼ í•˜ë¯€ë¡œ AWS ê³„ì • IDë¥¼ í¬í•¨í•©ë‹ˆë‹¤:

```bash
# AWS ê³„ì • ID ê°€ì ¸ì˜¤ê¸°
export AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)

# ë²„í‚· ì´ë¦„ ì„¤ì •
export S3_BUCKET_NAME=parallelcluster-${AWS_ACCOUNT_ID}-${AWS_REGION}

echo "S3 Bucket Name: ${S3_BUCKET_NAME}"
```

#### S3 ë²„í‚· ìƒì„±

```bash
# S3 ë²„í‚· ìƒì„±
aws s3 mb s3://${S3_BUCKET_NAME} --region ${AWS_REGION}
```

**ì˜ˆìƒ ì¶œë ¥:**
```
make_bucket: parallelcluster-123456789012-us-east-1
```

#### ë²„í‚· í™•ì¸

```bash
# ë²„í‚· ëª©ë¡ í™•ì¸
aws s3 ls | grep ${S3_BUCKET_NAME}
```

**ì˜ˆìƒ ì¶œë ¥:**
```
2024-01-01 12:00:00 parallelcluster-123456789012-us-east-1
```

---

### 5.2 ë¶€íŠ¸ìŠ¤íŠ¸ë© ìŠ¤í¬ë¦½íŠ¸ ì—…ë¡œë“œ

í´ëŸ¬ìŠ¤í„° ìƒì„± ì‹œ ìë™ìœ¼ë¡œ ì‹¤í–‰ë  ë¶€íŠ¸ìŠ¤íŠ¸ë© ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì—…ë¡œë“œí•©ë‹ˆë‹¤.

> ğŸ’¡ **ë¶€íŠ¸ìŠ¤íŠ¸ë© ìŠ¤í¬ë¦½íŠ¸ ì—­í• :**
> - **head-node-enroot-pyxis-setup.sh**: Head Nodeì—ì„œ Enrootì™€ Pyxis ì„¤ì¹˜ ë° ì„¤ì •
> - **compute-node-enroot-pyxis-setup.sh**: Compute Nodeì—ì„œ Enrootì™€ Pyxis ì„¤ì¹˜ ë° ì„¤ì •

#### ìŠ¤í¬ë¦½íŠ¸ ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±

```bash
# S3ì— ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±
aws s3api put-object \
  --bucket ${S3_BUCKET_NAME} \
  --key scripts/ \
  --region ${AWS_REGION}

aws s3api put-object \
  --bucket ${S3_BUCKET_NAME} \
  --key scripts/bootstrap/ \
  --region ${AWS_REGION}

#### ë¶€íŠ¸ìŠ¤íŠ¸ë© ìŠ¤í¬ë¦½íŠ¸ íŒŒì¼ í™•ì¸

ì—…ë¡œë“œí•  ìŠ¤í¬ë¦½íŠ¸ íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤:

```bash
# ë ˆí¬ì§€í† ë¦¬ ë£¨íŠ¸ë¡œ ì´ë™
cd ~/distributed-training-on-aws/pcluster-container

# ìŠ¤í¬ë¦½íŠ¸ íŒŒì¼ í™•ì¸
ls -lh examples/scripts/bootstrap/
```

**ì˜ˆìƒ ì¶œë ¥:**
```
-rwxr-xr-x  1 user  staff   3.2K  head-node-enroot-pyxis-setup.sh
-rwxr-xr-x  1 user  staff   2.8K  compute-node-enroot-pyxis-setup.sh
```

#### ë¶€íŠ¸ìŠ¤íŠ¸ë© ìŠ¤í¬ë¦½íŠ¸ ì—…ë¡œë“œ

```bash
# Head Node ìŠ¤í¬ë¦½íŠ¸ ì—…ë¡œë“œ
aws s3 cp examples/scripts/bootstrap/head-node-enroot-pyxis-setup.sh \
  s3://${S3_BUCKET_NAME}/scripts/bootstrap/ \
  --region ${AWS_REGION}

# Compute Node ìŠ¤í¬ë¦½íŠ¸ ì—…ë¡œë“œ
aws s3 cp examples/scripts/bootstrap/compute-node-enroot-pyxis-setup.sh \
  s3://${S3_BUCKET_NAME}/scripts/bootstrap/ \
  --region ${AWS_REGION}
```

**ì˜ˆìƒ ì¶œë ¥:**
```
upload: examples/scripts/bootstrap/head-node-enroot-pyxis-setup.sh to s3://parallelcluster-123456789012-us-east-1/scripts/bootstrap/head-node-enroot-pyxis-setup.sh
upload: examples/scripts/bootstrap/compute-node-enroot-pyxis-setup.sh to s3://parallelcluster-123456789012-us-east-1/scripts/bootstrap/compute-node-enroot-pyxis-setup.sh
```

#### ì—…ë¡œë“œëœ íŒŒì¼ í™•ì¸

```bash
# ë¶€íŠ¸ìŠ¤íŠ¸ë© ìŠ¤í¬ë¦½íŠ¸ í™•ì¸
aws s3 ls s3://${S3_BUCKET_NAME}/scripts/bootstrap/
```

**ì˜ˆìƒ ì¶œë ¥:**
```
2024-01-01 12:00:00       3276 head-node-enroot-pyxis-setup.sh
2024-01-01 12:00:00       2891 compute-node-enroot-pyxis-setup.sh
```

#### ìŠ¤í¬ë¦½íŠ¸ URL í™˜ê²½ ë³€ìˆ˜ ì €ì¥

ë‚˜ì¤‘ì— í´ëŸ¬ìŠ¤í„° ì„¤ì • íŒŒì¼ì—ì„œ ì‚¬ìš©í•  ìŠ¤í¬ë¦½íŠ¸ URLì„ í™˜ê²½ ë³€ìˆ˜ë¡œ ì €ì¥í•©ë‹ˆë‹¤:

```bash
# ë¶€íŠ¸ìŠ¤íŠ¸ë© ìŠ¤í¬ë¦½íŠ¸ URL
export HEAD_NODE_BOOTSTRAP_SCRIPT=s3://${S3_BUCKET_NAME}/scripts/bootstrap/head-node-enroot-pyxis-setup.sh
export COMPUTE_NODE_BOOTSTRAP_SCRIPT=s3://${S3_BUCKET_NAME}/scripts/bootstrap/compute-node-enroot-pyxis-setup.sh

# í™•ì¸
echo "Head Node Bootstrap: ${HEAD_NODE_BOOTSTRAP_SCRIPT}"
echo "Compute Node Bootstrap: ${COMPUTE_NODE_BOOTSTRAP_SCRIPT}"
```

**ì˜ˆìƒ ì¶œë ¥:**
```
Head Node Bootstrap: s3://parallelcluster-123456789012-us-east-1/scripts/bootstrap/head-node-enroot-pyxis-setup.sh
Compute Node Bootstrap: s3://parallelcluster-123456789012-us-east-1/scripts/bootstrap/compute-node-enroot-pyxis-setup.sh
```

#### í™˜ê²½ ë³€ìˆ˜ íŒŒì¼ì— ì¶”ê°€

```bash
# í™˜ê²½ ë³€ìˆ˜ íŒŒì¼ì— ì¶”ê°€
cat >> ~/pcluster-env.sh << EOF
export HEAD_NODE_BOOTSTRAP_SCRIPT=${HEAD_NODE_BOOTSTRAP_SCRIPT}
export COMPUTE_NODE_BOOTSTRAP_SCRIPT=${COMPUTE_NODE_BOOTSTRAP_SCRIPT}
EOF
```

> ğŸ“ **ì°¸ê³ :** 
> - ì´ ìŠ¤í¬ë¦½íŠ¸ë“¤ì€ ë‹¤ìŒ ë‹¨ê³„ì¸ í´ëŸ¬ìŠ¤í„° ë°°í¬ ì‹œ ìë™ìœ¼ë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.
> - Head Nodeì™€ Compute Nodeì— ê°ê° Enrootì™€ Pyxisê°€ ì„¤ì¹˜ë˜ì–´ ì»¨í…Œì´ë„ˆ ê¸°ë°˜ ì‘ì—…ì„ ì‹¤í–‰í•  ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤.

---

### 5.3 í•™ìŠµ ë°ì´í„°ì…‹ ì¤€ë¹„

í•™ìŠµì— ì‚¬ìš©í•  ë°ì´í„°ì…‹ì„ S3ì— ì—…ë¡œë“œí•©ë‹ˆë‹¤. ì´ ë°ì´í„°ëŠ” FSx Lustreë¥¼ í†µí•´ ê³ ì„±ëŠ¥ìœ¼ë¡œ ì ‘ê·¼í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

#### S3 ë””ë ‰í† ë¦¬ êµ¬ì¡°

í•™ìŠµ ì›Œí¬í”Œë¡œìš°ì— ë§ì¶° S3ì— ë‹¤ìŒê³¼ ê°™ì€ ë””ë ‰í† ë¦¬ êµ¬ì¡°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤:

```
s3://parallelcluster-{account-id}-{region}/
â”œâ”€â”€ data/              # í•™ìŠµ ë°ì´í„°ì…‹
â”œâ”€â”€ checkpoints/       # ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ (í•™ìŠµ ì¤‘ ì €ì¥)
â”œâ”€â”€ logs/              # í•™ìŠµ ë¡œê·¸
â”œâ”€â”€ results/           # ìµœì¢… ê²°ê³¼ ë° ëª¨ë¸
â””â”€â”€ scripts/           # ParallelCluster Node ê´€ë ¨ ë¶€íŠ¸ìŠ¤íŠ¸ë© ìŠ¤í¬ë¦½íŠ¸ (ì´ë¯¸ ìƒì„±ë¨)
```

#### ë””ë ‰í† ë¦¬ ìƒì„±

```bash
# S3ì— ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±
aws s3api put-object --bucket ${S3_BUCKET_NAME} --key data/ --region ${AWS_REGION}
aws s3api put-object --bucket ${S3_BUCKET_NAME} --key checkpoints/ --region ${AWS_REGION}
aws s3api put-object --bucket ${S3_BUCKET_NAME} --key logs/ --region ${AWS_REGION}
aws s3api put-object --bucket ${S3_BUCKET_NAME} --key results/ --region ${AWS_REGION}
```

#### ìƒ˜í”Œ ë°ì´í„°ì…‹ ì—…ë¡œë“œ

```bash
# ì„ì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
mkdir -p /tmp/sample-data

# ìƒ˜í”Œ ë°ì´í„° íŒŒì¼ ìƒì„±
cat > /tmp/sample-data/README.txt << 'EOF'
Sample Training Dataset
=======================
This directory contains sample training data.
Replace this with your actual dataset.
EOF

# S3ì— ì—…ë¡œë“œ
aws s3 cp /tmp/sample-data/ \
  s3://${S3_BUCKET_NAME}/data/sample/ \
  --recursive \
  --region ${AWS_REGION}

# ì •ë¦¬
rm -rf /tmp/sample-data
```

#### ì‹¤ì œ ë°ì´í„°ì…‹ ì—…ë¡œë“œ ì˜ˆì‹œ

**ëŒ€ìš©ëŸ‰ ë°ì´í„°ì…‹ ì—…ë¡œë“œ:**
```bash
# ë¡œì»¬ ë°ì´í„°ì…‹ ë””ë ‰í† ë¦¬ë¥¼ S3ë¡œ ì—…ë¡œë“œ
aws s3 sync /path/to/your/dataset/ \
  s3://${S3_BUCKET_NAME}/data/imagenet/ \
  --region ${AWS_REGION}
```

**Hugging Face ë°ì´í„°ì…‹:**
```bash
# Python ìŠ¤í¬ë¦½íŠ¸ë¡œ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ë° S3 ì—…ë¡œë“œ
python3 << 'EOF'
from datasets import load_dataset
import boto3
import os

# ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ
dataset = load_dataset("wikitext", "wikitext-2-raw-v1", split="train[:1000]")

# ë¡œì»¬ì— ì €ì¥
output_dir = "/tmp/wikitext-sample"
dataset.save_to_disk(output_dir)

# S3ë¡œ ì—…ë¡œë“œ
s3 = boto3.client('s3')
bucket_name = os.environ['S3_BUCKET_NAME']

for root, dirs, files in os.walk(output_dir):
    for file in files:
        local_path = os.path.join(root, file)
        s3_path = local_path.replace(output_dir, 'data/wikitext').lstrip('/')
        s3.upload_file(local_path, bucket_name, s3_path)
        print(f"Uploaded: {s3_path}")

print("Dataset upload completed!")
EOF
```

#### ì—…ë¡œë“œëœ ë°ì´í„° í™•ì¸

```bash
# S3 ë°ì´í„° í™•ì¸
aws s3 ls s3://${S3_BUCKET_NAME}/data/ --recursive --human-readable
```

**ì˜ˆìƒ ì¶œë ¥:**
```
2024-01-01 12:00:00    1.2 KiB data/sample/README.txt
2024-01-01 12:05:00   10.5 MiB data/wikitext/dataset_info.json
```

---

### 5.4 FSx Lustreì™€ S3 ì—°ë™ ì„¤ì •

FSx Lustreê°€ S3 ë°ì´í„°ë¥¼ ìë™ìœ¼ë¡œ ê°€ì ¸ì˜¤ê³  ë‚´ë³´ë‚¼ ìˆ˜ ìˆë„ë¡ Data Repository Association (DRA)ì„ ì„¤ì •í•©ë‹ˆë‹¤.

> ğŸ’¡ **Data Repository Association (DRA)ì´ë€?**
> - FSx Lustreì™€ S3 ë²„í‚· ê°„ì˜ ì—°ê²°ì„ ì„¤ì •
> - S3ì˜ ë°ì´í„°ë¥¼ FSxë¡œ ìë™ import (Lazy Loading)
> - FSxì˜ ë³€ê²½ì‚¬í•­ì„ S3ë¡œ ìë™ export (ë°±ì—…)
> - ì—¬ëŸ¬ ê°œì˜ S3 ê²½ë¡œë¥¼ FSxì˜ ë‹¤ë¥¸ ê²½ë¡œì— ë§¤í•‘ ê°€ëŠ¥

#### FSx Lustre ë””ë ‰í† ë¦¬ êµ¬ì¡°

FSx Lustreì—ì„œ ë‹¤ìŒê³¼ ê°™ì€ ë””ë ‰í† ë¦¬ êµ¬ì¡°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤:

```
/lustre/
â”œâ”€â”€ data/              # S3 data/ ì™€ ì—°ë™ (í•™ìŠµ ë°ì´í„°)
â”œâ”€â”€ checkpoints/       # S3 checkpoints/ ì™€ ì—°ë™ (ì²´í¬í¬ì¸íŠ¸ ì €ì¥/ë³µì›)
â”œâ”€â”€ logs/              # S3 logs/ ì™€ ì—°ë™ (í•™ìŠµ ë¡œê·¸)
â””â”€â”€ results/           # S3 results/ ì™€ ì—°ë™ (ìµœì¢… ê²°ê³¼)
```

#### DRA í™˜ê²½ ë³€ìˆ˜ ì„¤ì •

```bash
# DRA ì´ë¦„ ë° ê²½ë¡œ ì„¤ì •
export DRA_DATA_NAME=training-data
export DRA_CHECKPOINTS_NAME=training-checkpoints
export DRA_LOGS_NAME=training-logs
export DRA_RESULTS_NAME=training-results
```

#### Data Repository Association ìƒì„±

**1. í•™ìŠµ ë°ì´í„°ìš© DRA:**
```bash
aws fsx create-data-repository-association \
  --file-system-id ${FSX_LUSTRE_ID} \
  --file-system-path /data \
  --data-repository-path s3://${S3_BUCKET_NAME}/data/ \
  --batch-import-meta-data-on-create \
  --s3 '{
    "AutoImportPolicy": {
      "Events": ["NEW", "CHANGED", "DELETED"]
    }
  }' \
  --region ${AWS_REGION}
```

**2. ì²´í¬í¬ì¸íŠ¸ìš© DRA:**
```bash
aws fsx create-data-repository-association \
  --file-system-id ${FSX_LUSTRE_ID} \
  --file-system-path /checkpoints \
  --data-repository-path s3://${S3_BUCKET_NAME}/checkpoints/ \
  --s3 '{
    "AutoImportPolicy": {
      "Events": ["NEW", "CHANGED", "DELETED"]
    },
    "AutoExportPolicy": {
      "Events": ["NEW", "CHANGED", "DELETED"]
    }
  }' \
  --region ${AWS_REGION}
```

**3. ë¡œê·¸ìš© DRA:**
```bash
aws fsx create-data-repository-association \
  --file-system-id ${FSX_LUSTRE_ID} \
  --file-system-path /logs \
  --data-repository-path s3://${S3_BUCKET_NAME}/logs/ \
  --s3 '{
    "AutoExportPolicy": {
      "Events": ["NEW", "CHANGED", "DELETED"]
    }
  }' \
  --region ${AWS_REGION}
```

**4. ê²°ê³¼ìš© DRA:**
```bash
aws fsx create-data-repository-association \
  --file-system-id ${FSX_LUSTRE_ID} \
  --file-system-path /results \
  --data-repository-path s3://${S3_BUCKET_NAME}/results/ \
  --s3 '{
    "AutoExportPolicy": {
      "Events": ["NEW", "CHANGED", "DELETED"]
    }
  }' \
  --region ${AWS_REGION}
```

**ì˜ˆìƒ ì¶œë ¥ (ê° DRAë§ˆë‹¤):**
```json
{
    "Association": {
        "AssociationId": "dra-0a1b2c3d4e5f6g7h8",
        "ResourceARN": "arn:aws:fsx:us-east-1:123456789012:association/fs-xxx/dra-xxx",
        "FileSystemId": "fs-0a1b2c3d4e5f6g7h8",
        "Lifecycle": "CREATING",
        "FileSystemPath": "/data",
        "DataRepositoryPath": "s3://parallelcluster-123456789012-us-east-1/data/",
        "BatchImportMetaDataOnCreate": true,
        "ImportedFileChunkSize": 1024,
        "S3": {
            "AutoImportPolicy": {
                "Events": ["NEW", "CHANGED", "DELETED"]
            }
        }
    }
}
```

> ğŸ’¡ **DRA ì„¤ì • ì„¤ëª…:**
> - **AutoImportPolicy**: S3ì—ì„œ FSxë¡œ ìë™ ê°€ì ¸ì˜¤ê¸°
>   - `data/`: í•™ìŠµ ë°ì´í„°ëŠ” importë§Œ (ì½ê¸° ì „ìš©)
>   - `checkpoints/`: import & export (ì €ì¥ ë° ë³µì›)
> - **AutoExportPolicy**: FSxì—ì„œ S3ë¡œ ìë™ ë‚´ë³´ë‚´ê¸°
>   - `checkpoints/`, `logs/`, `results/`: FSxì—ì„œ ìƒì„±ëœ íŒŒì¼ì„ S3ë¡œ ë°±ì—…
> - **BatchImportMetaDataOnCreate**: ìƒì„± ì‹œ S3 ë©”íƒ€ë°ì´í„° ì¼ê´„ ê°€ì ¸ì˜¤ê¸°

#### DRA ìƒì„± ìƒíƒœ í™•ì¸

```bash
# ëª¨ë“  DRA ëª©ë¡ í™•ì¸
aws fsx describe-data-repository-associations \
  --filters Name=file-system-id,Values=${FSX_LUSTRE_ID} \
  --region ${AWS_REGION} \
  --query 'Associations[*].[AssociationId,FileSystemPath,DataRepositoryPath,Lifecycle]' \
  --output table
```

**ì˜ˆìƒ ì¶œë ¥:**
```
---------------------------------------------------------------
|          DescribeDataRepositoryAssociations                |
+----------------------+---------------+-----------+-----------+
|  dra-0a1b2c3d...    |  /data        | s3://.../data/       | AVAILABLE |
|  dra-1b2c3d4e...    |  /checkpoints | s3://.../checkpoints/| AVAILABLE |
|  dra-2c3d4e5f...    |  /logs        | s3://.../logs/       | AVAILABLE |
|  dra-3d4e5f6g...    |  /results     | s3://.../results/    | AVAILABLE |
+----------------------+---------------+-----------+-----------+
```

#### DRA ìƒíƒœê°€ AVAILABLEì´ ë  ë•Œê¹Œì§€ ëŒ€ê¸°

```bash
# DRA ìƒì„± ì™„ë£Œ í™•ì¸ (ëª¨ë“  DRAê°€ AVAILABLE ìƒíƒœê°€ ë  ë•Œê¹Œì§€)
while true; do
  STATUS=$(aws fsx describe-data-repository-associations \
    --filters Name=file-system-id,Values=${FSX_LUSTRE_ID} \
    --region ${AWS_REGION} \
    --query 'Associations[?Lifecycle!=`AVAILABLE`].Lifecycle' \
    --output text)
  
  if [ -z "$STATUS" ]; then
    echo "âœ… All DRAs are AVAILABLE!"
    break
  else
    echo "Waiting for DRAs to be AVAILABLE... (Current: $STATUS)"
    sleep 30
  fi
done
```

> â±ï¸ **ì˜ˆìƒ ì†Œìš” ì‹œê°„:** ê° DRAë‹¹ 1-2ë¶„, ì´ 5-10ë¶„

#### ë°ì´í„° ì ‘ê·¼ ì˜ˆì‹œ

DRA ì„¤ì •ì´ ì™„ë£Œë˜ë©´ í´ëŸ¬ìŠ¤í„°ì—ì„œ ë‹¤ìŒê³¼ ê°™ì´ ë°ì´í„°ì— ì ‘ê·¼í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```bash
# í´ëŸ¬ìŠ¤í„° Head Nodeì—ì„œ ì‹¤í–‰ (í´ëŸ¬ìŠ¤í„° ìƒì„± í›„)
# S3: s3://bucket/data/imagenet/train/image001.jpg
# FSx: /lustre/data/imagenet/train/image001.jpg

# S3ì—ì„œ FSxë¡œ ìë™ import (ì²« ì ‘ê·¼ ì‹œ)
ls /lustre/data/imagenet/

# ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (FSx â†’ S3ë¡œ ìë™ export)
cp model.pth /lustre/checkpoints/epoch_10.pth

# ë¡œê·¸ ì €ì¥ (FSx â†’ S3ë¡œ ìë™ export)
echo "Training completed" > /lustre/logs/training.log
```

#### FSx Lustre ë””ë ‰í† ë¦¬ í™˜ê²½ ë³€ìˆ˜ ì €ì¥

ë‚˜ì¤‘ì— í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ê²½ë¡œë¥¼ ì €ì¥í•©ë‹ˆë‹¤:

```bash
# FSx Lustre ê²½ë¡œ í™˜ê²½ ë³€ìˆ˜
export LUSTRE_DATA_DIR=/lustre/data
export LUSTRE_CHECKPOINT_DIR=/lustre/checkpoints
export LUSTRE_LOG_DIR=/lustre/logs
export LUSTRE_RESULTS_DIR=/lustre/results

# í™˜ê²½ ë³€ìˆ˜ íŒŒì¼ì— ì¶”ê°€
cat >> ~/pcluster-env.sh << EOF
export LUSTRE_DATA_DIR=${LUSTRE_DATA_DIR}
export LUSTRE_CHECKPOINT_DIR=${LUSTRE_CHECKPOINT_DIR}
export LUSTRE_LOG_DIR=${LUSTRE_LOG_DIR}
export LUSTRE_RESULTS_DIR=${LUSTRE_RESULTS_DIR}
EOF
```

---

### 5.5 í™˜ê²½ ë³€ìˆ˜ ì €ì¥

S3 ë²„í‚· ë° ì´ë¯¸ì§€ ì •ë³´ë¥¼ í™˜ê²½ ë³€ìˆ˜ íŒŒì¼ì— ì¶”ê°€í•©ë‹ˆë‹¤:

```bash
# í™˜ê²½ ë³€ìˆ˜ íŒŒì¼ì— ì¶”ê°€
cat >> ~/pcluster-env.sh << EOF
export AWS_ACCOUNT_ID=${AWS_ACCOUNT_ID}
export S3_BUCKET_NAME=${S3_BUCKET_NAME}
export ECR_REPO_NAME=${ECR_REPO_NAME}
export ECR_REPO_URI=${ECR_REPO_URI}
export IMAGE_TAG=${IMAGE_TAG}
export TRAINING_IMAGE_URI=${TRAINING_IMAGE_URI}
EOF

# í™•ì¸
source ~/pcluster-env.sh
```

#### ì „ì²´ í™˜ê²½ ë³€ìˆ˜ í™•ì¸

```bash
# ì €ì¥ëœ ëª¨ë“  í™˜ê²½ ë³€ìˆ˜ í™•ì¸
cat ~/pcluster-env.sh
```

**ì˜ˆìƒ ì¶œë ¥:**
```bash
export AWS_REGION=us-east-1
export STACK_NAME=parallelcluster-prerequisites
export PRIMARY_AZ=us-east-1a
export VPC_ID=vpc-0a1b2c3d4e5f6g7h8
export PUBLIC_SUBNET_ID=subnet-0a1b2c3d
export PRIVATE_SUBNET_ID=subnet-4e5f6g7h
export SECURITY_GROUP_ID=sg-0a1b2c3d4e5f6g7h8
export FSX_LUSTRE_ID=fs-0a1b2c3d4e5f6g7h8
export FSX_LUSTRE_MOUNT_NAME=xxxxxxxx
export FSX_LUSTRE_DNS=fs-xxx.fsx.us-east-1.amazonaws.com
export FSX_OPENZFS_ROOT_VOLUME_ID=fsvol-0a1b2c3d4e5f6g7h8
export HEAD_NODE_BOOTSTRAP_SCRIPT=s3://parallelcluster-123456789012-us-east-1/scripts/bootstrap/head-node-enroot-pyxis-setup.sh
export COMPUTE_NODE_BOOTSTRAP_SCRIPT=s3://parallelcluster-123456789012-us-east-1/scripts/bootstrap/compute-node-enroot-pyxis-setup.sh
export AWS_ACCOUNT_ID=123456789012
export S3_BUCKET_NAME=parallelcluster-123456789012-us-east-1
export ECR_REPO_NAME=pytorch-training-custom
export ECR_REPO_URI=123456789012.dkr.ecr.us-east-1.amazonaws.com/pytorch-training-custom
export IMAGE_TAG=latest
export TRAINING_IMAGE_URI=123456789012.dkr.ecr.us-east-1.amazonaws.com/pytorch-training-custom:latest
export LUSTRE_DATA_DIR=/lustre/data
export LUSTRE_CHECKPOINT_DIR=/lustre/checkpoints
export LUSTRE_LOG_DIR=/lustre/logs
export LUSTRE_RESULTS_DIR=/lustre/results
```

---

### 5.6 S3 ë²„í‚· êµ¬ì¡° ìµœì¢… í™•ì¸

```bash
# ì „ì²´ ë²„í‚· êµ¬ì¡° í™•ì¸
aws s3 ls s3://${S3_BUCKET_NAME}/ --recursive --human-readable --summarize
```

**ì˜ˆìƒ êµ¬ì¡°:**
```
2024-01-01 12:00:00    3.2 KiB scripts/bootstrap/head-node-enroot-pyxis-setup.sh
2024-01-01 12:00:00    2.8 KiB scripts/bootstrap/compute-node-enroot-pyxis-setup.sh
2024-01-01 12:00:00    1.2 KiB data/sample/README.txt
                           PRE checkpoints/
                           PRE logs/
                           PRE results/

Total Objects: 3
   Total Size: 7.2 KiB
```

---

## ë‹¤ìŒ ë‹¨ê³„

âœ… ì‚¬ì „ ìš”êµ¬ì‚¬í•­ ì¤€ë¹„ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!

ì´ì œ **[2. ParallelCluster ë°°í¬](./02-pcluster-deployment.md)** ë¡œ ì§„í–‰í•˜ì—¬ í´ëŸ¬ìŠ¤í„°ë¥¼ ìƒì„±í•˜ì„¸ìš”.

---

## ğŸ“š ë„¤ë¹„ê²Œì´ì…˜

| ì´ì „ | ìƒìœ„ | ë‹¤ìŒ |
|------|------|------|
| [â—€ README](../README.md) | [ğŸ“‘ ëª©ì°¨](../README.md#-ê°€ì´ë“œ-ëª©ì°¨) | [í´ëŸ¬ìŠ¤í„° ë°°í¬ â–¶](./02-pcluster-deployment.md) |
