# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: MIT-0
Region: ${AWS_REGION}

DevSettings:
  Timeouts:
    HeadNodeBootstrapTimeout: 43200  # 12 hours
    ComputeNodeBootstrapTimeout: 7200  # 2 hours

Imds:
  ImdsSupport: v2.0

Image:
  Os: ubuntu2204

HeadNode:
  InstanceType: m5.8xlarge
  Networking:
    SubnetId: ${PUBLIC_SUBNET_ID}
    ElasticIp: false
    AdditionalSecurityGroups:
      - ${SECURITY_GROUP_ID}
  LocalStorage:
    RootVolume:
      Size: 500
      DeleteOnTermination: true  # Root and /home volume for users
  Iam:
    AdditionalIamPolicies:
      # Grant ECR, SSM and S3 access
      - Policy: arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore
      - Policy: arn:aws:iam::aws:policy/AmazonS3FullAccess
      - Policy: arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryFullAccess
      - Policy: arn:aws:iam::aws:policy/AmazonElasticContainerRegistryPublicFullAccess
  CustomActions:
    OnNodeConfigured:
      Sequence:
        - Script: 'https://raw.githubusercontent.com/aws-samples/aws-parallelcluster-post-install-scripts/main/docker/postinstall.sh'
        - Script: '${HEAD_NODE_BOOTSTRAP_SCRIPT}'

Scheduling:
  Scheduler: slurm
  SlurmSettings:
    ScaledownIdletime: -1  # Disable automatic scale-down
    QueueUpdateStrategy: DRAIN
    CustomSlurmSettings:
      # Simple accounting to text file
      - JobCompType: jobcomp/filetxt
      - JobCompLoc: /home/slurm/slurm-job-completions.txt
      - JobAcctGatherType: jobacct_gather/linux
      # Increase timeout before marking node as DOWN
      - SlurmdTimeout: 1000
  SlurmQueues:
    - Name: compute-gpu
      CapacityType: ONDEMAND
      Networking:
        SubnetIds:
          - ${PRIVATE_SUBNET_ID}
        PlacementGroup:
          Enabled: true  # Capacity Reservation 사용 시 false로 변경
        AdditionalSecurityGroups:
          - ${SECURITY_GROUP_ID}
      ComputeSettings:
        LocalStorage:
          EphemeralVolume:
            MountDir: /scratch  # Local NVMe scratch space
          RootVolume:
            Size: 512
      JobExclusiveAllocation: true  # Each job gets exclusive access to nodes
      ComputeResources:
        - Name: distributed-ml
          InstanceType: ${COMPUTE_INSTANCE_TYPE}
          MinCount: ${COMPUTE_MIN_COUNT}
          MaxCount: ${COMPUTE_MAX_COUNT}
          # Capacity Reservation 사용 시 아래 주석 해제
          # CapacityReservationTarget:
          #   CapacityReservationId: cr-0a1f6b92ded769450  # Replace with your Capacity Reservation ID
          Efa:
            Enabled: true
            #GdrSupport: true  # p4d/p5 인스턴스만 지원
      Iam:
        AdditionalIamPolicies:
          - Policy: arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore
          - Policy: arn:aws:iam::aws:policy/AmazonS3FullAccess
          - Policy: arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryFullAccess
          - Policy: arn:aws:iam::aws:policy/AmazonElasticContainerRegistryPublicFullAccess
      CustomActions:
        OnNodeConfigured:
          Sequence:
            - Script: 'https://raw.githubusercontent.com/aws-samples/aws-parallelcluster-post-install-scripts/main/docker/postinstall.sh'
            - Script: '${COMPUTE_NODE_BOOTSTRAP_SCRIPT}'

SharedStorage:
  - Name: shared-workspace-zfs
    StorageType: FsxOpenZfs
    MountDir: /fsx
    FsxOpenZfsSettings:
      VolumeId: ${FSX_OPENZFS_ROOT_VOLUME_ID}

  - Name: fsx-lustre
    MountDir: /lustre
    StorageType: FsxLustre
    FsxLustreSettings:
      FileSystemId: ${FSX_LUSTRE_ID}
      DeploymentType: PERSISTENT_1

Monitoring:
  DetailedMonitoring: true
  Logs:
    CloudWatch:
      Enabled: true
  Dashboards:
    CloudWatch:
      Enabled: true